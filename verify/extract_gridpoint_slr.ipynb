{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "import pickle\n",
    "import cfgrib\n",
    "import pygrib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "mp_use_cores = 32\n",
    "use_era_scaler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gfs0p25'\n",
    "archive = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/archive/'\n",
    "mlmodel_dir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/output/slr_models/all_dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fmt = '%Y%m%d'\n",
    "datetime_fmt = '%Y%m%d%H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_lat, site_lon = 40.5763, -111.6383\n",
    "\n",
    "gfs_sample = xr.open_dataset('./gfs_latlon_grid.nc')\n",
    "gfs_sample['longitude'] = gfs_sample['longitude'] - 360\n",
    "gfs_lat, gfs_lon = gfs_sample['latitude'], gfs_sample['longitude']\n",
    "\n",
    "idx1d = (np.abs(gfs_lon - site_lon) + np.abs(gfs_lat - site_lat))\n",
    "lon_idx, lat_idx = np.where(idx1d == np.min(idx1d))\n",
    "lon_idx, lat_idx = lon_idx[0], lat_idx[0]\n",
    "lon_idx, lat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_gfs(f):\n",
    "    \n",
    "    # print('Reading %s'%os.path.basename(f))\n",
    "\n",
    "    datasets = cfgrib.open_datasets(f)\n",
    "\n",
    "    keep_keys = ['tp', 'q', 't', 'u', 'v', 'absv', 'w', 'gh', 'r', 'd', \n",
    "                  'u10', 'v10', 'u100', 'v100', 't2m', 'd2m', \n",
    "                  'cape', 'prmsl', 'sp', 'orog', 'hpbl']\n",
    "\n",
    "    sfc, iso = [], []\n",
    "\n",
    "    for ds in datasets:\n",
    "        \n",
    "        ds = ds.isel(latitude=lat_idx, longitude=lon_idx).load()\n",
    "\n",
    "        key_match = np.array(list(ds.data_vars))[np.isin(list(ds.data_vars), keep_keys)]\n",
    "\n",
    "        if len(key_match) > 0:\n",
    "\n",
    "            dims = ds.dims.keys()\n",
    "            coords = ds[key_match].coords\n",
    "\n",
    "            if ('heightAboveGround' in coords) & ('heightAboveGround' not in dims):\n",
    "                sfc.append(ds[key_match].drop('heightAboveGround'))\n",
    "\n",
    "            elif 'isobaricInhPa' in coords:\n",
    "                iso.append(ds[key_match])\n",
    "\n",
    "            elif (('surface' in coords)|('meanSea' in coords)):\n",
    "                sfc.append(ds[key_match])\n",
    "\n",
    "            elif 'prmsl' in list(ds.data_vars):\n",
    "                sfc.append(ds['prmsl'])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    sfc = xr.merge(sfc).drop('t')\n",
    "    iso = xr.merge(iso).rename({'isobaricInhPa':'level'})\n",
    "    iso = iso.sel(level=iso.level[::-1])\n",
    "\n",
    "    sfc['longitude'] = sfc['longitude'] - 360\n",
    "    iso['longitude'] = iso['longitude'] - 360\n",
    "    \n",
    "    return [sfc.drop(['surface', 'meanSea', 'step']), \n",
    "            iso.drop('step')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 24\n",
    "valid = datetime(2017, 1, 26, 0, 0)\n",
    "init = valid - timedelta(hours=interval)\n",
    "\n",
    "f0 = 24 - interval + 3\n",
    "f1, fi = 24, 3\n",
    "fhrs = ['f%03d'%i for i in np.arange(f0, f1+1, fi)]\n",
    "\n",
    "flist = glob(archive + init.strftime(date_fmt) + \n",
    "             '/models/%s/*%s*.grib2'%(model, init.strftime(datetime_fmt)))[1:]\n",
    "\n",
    "flist = [f for f in flist if f.split('.')[-3] in fhrs]\n",
    "flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mp.get_context('fork').Pool(mp_use_cores) as p:\n",
    "#     returns = p.map(ingest_gfs, flist, chunksize=1)\n",
    "#     p.close()\n",
    "#     p.join()\n",
    "\n",
    "returns = [ingest_gfs(f) for f in flist]\n",
    "\n",
    "returns = np.array(returns, dtype=object)\n",
    "sfc, iso = returns[:, 0], returns[:, 1]\n",
    "    \n",
    "iso = xr.concat(iso, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')\n",
    "sfc = xr.concat(sfc, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = iso['u'], iso['v']\n",
    "wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "\n",
    "iso['dir'] = wdir\n",
    "iso['spd'] = np.sqrt(u**2 + v**2)\n",
    "\n",
    "for hgt in [10, 100]:\n",
    "    \n",
    "    u, v = sfc['u%d'%hgt], sfc['v%d'%hgt]\n",
    "    wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "    wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "    wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "    \n",
    "    sfc['dir%dm'%hgt] = wdir\n",
    "    sfc['spd%dm'%hgt] = np.sqrt(u**2 + v**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orog = sfc.orog\n",
    "gh = iso.gh\n",
    "\n",
    "lowest_level = np.full(orog.shape, fill_value=np.nan)\n",
    "lowest_level_index = np.full(orog.shape, fill_value=np.nan)\n",
    "\n",
    "for i, level in enumerate(iso['level']):\n",
    "    \n",
    "    lev_gh = gh.sel(level=level)\n",
    "    lowest_level = xr.where(orog >= lev_gh, level.values, lowest_level)\n",
    "    lowest_level_index = xr.where(orog >= lev_gh, i, lowest_level_index)\n",
    "    \n",
    "lowest_level_index = xr.where(np.isnan(lowest_level), 0, lowest_level_index)\n",
    "lowest_level = xr.where(np.isnan(lowest_level), 1000, lowest_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "match_rename = {'absv':'vo', 'gh':'z', 'hpbl':'blh', 'prmsl':'msl', 'tp':'swe_mm',\n",
    "               'u10':'u10m', 'v10':'v10m', 'u100':'u100m', 'v100':'v100m'}\n",
    "\n",
    "# Loop over each variable in the xarray\n",
    "for ds in [iso, sfc.drop('orog')]:\n",
    "    \n",
    "    for var_name in ds.data_vars:\n",
    "        \n",
    "        new_var_name = match_rename[var_name] if var_name in match_rename.keys() else var_name\n",
    "        print('Reducing (%s) to %s index level AGL'%(var_name, new_var_name))\n",
    "\n",
    "        var = ds[var_name]\n",
    "\n",
    "        if 'level' in var.coords:\n",
    "\n",
    "            for i in np.arange(10):\n",
    "\n",
    "                var_agl = np.full(shape=(orog.shape), fill_value=np.nan)\n",
    "\n",
    "                for j, level in enumerate(iso['level']):\n",
    "\n",
    "                    var_agl = xr.where(lowest_level_index+i == j, var.isel(level=j), var_agl)\n",
    "\n",
    "                    # Record the levels used, should match lowest_level array, sanity check\n",
    "                    # var_agl[i, :, :] = xr.where(lowest_level_index+i == j, level, var_agl[i, :, :])\n",
    "\n",
    "                # We could ho ahead and append to the pandas dataframe here \n",
    "                # at the completion of each level (_01agl, _02agl...)\n",
    "                # We will have to use [(time), lat, lon] as a multiindex\n",
    "                var_agl = xr.DataArray(var_agl, \n",
    "                     dims=['time'], \n",
    "                     coords={'time':ds['time'],\n",
    "                             'latitude':ds['latitude'], \n",
    "                             'longitude':ds['longitude']})\n",
    "\n",
    "                df.append(var_agl.to_dataframe(name='%s_%02dagl'%(new_var_name.upper(), i+1)))\n",
    "\n",
    "                del var_agl\n",
    "                gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            var_agl = xr.DataArray(var.values, \n",
    "                dims=['time'], \n",
    "                coords={'time':ds['time'],\n",
    "                    'latitude':ds['latitude'], \n",
    "                     'longitude':ds['longitude']})\n",
    "\n",
    "            df.append(var_agl.to_dataframe(name='%s'%new_var_name.upper()))\n",
    "\n",
    "# SLOW!!! Is there anything we can do here??\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=['time', 'latitude', 'longitude']), df)\n",
    "df = df.rename(columns={'SWE_MM':'swe_mm'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_file = glob(mlmodel_dir + '*scaler*')[-1]\n",
    "stats_file = glob(mlmodel_dir + '*train_stats*')[-1]\n",
    "model_file = glob(mlmodel_dir + '*SLRmodel*')[-1]\n",
    "\n",
    "if use_era_scaler == True:\n",
    "    with open(scaler_file, 'rb') as rfp:\n",
    "        scaler = pickle.load(rfp)\n",
    "else:\n",
    "    scaler = RobustScaler(quantile_range=(25, 75))\n",
    "\n",
    "with open(stats_file, 'rb') as rfp:\n",
    "    train_stats, train_stats_norm = pickle.load(rfp)\n",
    "    model_keys = train_stats.keys()\n",
    "    \n",
    "with open(model_file, 'rb') as rfp:\n",
    "    SLRmodel = pickle.load(rfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, model_keys]\n",
    "scaler = scaler.fit(df)\n",
    "\n",
    "df_norm = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.keys())\n",
    "df_norm\n",
    "\n",
    "print('\\ncheck: missing from model', [k for k in df.keys() if k not in model_keys])\n",
    "print('\\ncheck: missing from input', [k for k in model_keys if k not in df.keys()])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to need to bin these out and process in parallel\n",
    "# By time is likely easiest\n",
    "\n",
    "slr = pd.DataFrame(SLRmodel.predict(df_norm), \n",
    "                   index=df_norm.index, columns=['slr']\n",
    "                  ).to_xarray()['slr']\n",
    "\n",
    "slr = xr.where(slr < 0, 0, slr)\n",
    "\n",
    "slr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = df['swe_mm']\n",
    "snow = swe * slr\n",
    "slr_weighted = round(snow.sum()/swe.sum(), 1)\n",
    "\n",
    "valid = pd.to_datetime(slr[-1].time.values)\n",
    "valid, slr_weighted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
