{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "import pickle\n",
    "import cfgrib\n",
    "import pygrib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "mp_use_cores = 32\n",
    "use_era_scaler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_lat, site_lon = 40.5763, -111.6383\n",
    "interval, valid = 24, datetime(2017, 1, 26, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gfs0p25'\n",
    "archive = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/archive/'\n",
    "mlmodel_dir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/output/slr_models/all_dev/'\n",
    "\n",
    "date_fmt = '%Y%m%d'\n",
    "datetime_fmt = '%Y%m%d%H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_gfs(f):\n",
    "    \n",
    "    datasets = cfgrib.open_datasets(f)\n",
    "\n",
    "    keep_keys = ['tp', 'q', 't', 'u', 'v', 'absv', 'w', 'gh', 'r', 'd', \n",
    "                  'u10', 'v10', 'u100', 'v100', 't2m', 'd2m', \n",
    "                  'cape', 'prmsl', 'sp', 'orog', 'hpbl']\n",
    "\n",
    "    sfc, iso = [], []\n",
    "\n",
    "    for ds in datasets:\n",
    "        \n",
    "        ds = ds.isel(latitude=lat_idx, longitude=lon_idx).load()\n",
    "\n",
    "        key_match = np.array(list(ds.data_vars))[np.isin(list(ds.data_vars), keep_keys)]\n",
    "\n",
    "        if len(key_match) > 0:\n",
    "\n",
    "            dims = ds.dims.keys()\n",
    "            coords = ds[key_match].coords\n",
    "\n",
    "            if ('heightAboveGround' in coords) & ('heightAboveGround' not in dims):\n",
    "                sfc.append(ds[key_match].drop('heightAboveGround'))\n",
    "\n",
    "            elif 'isobaricInhPa' in coords:\n",
    "                iso.append(ds[key_match])\n",
    "\n",
    "            elif (('surface' in coords)|('meanSea' in coords)):\n",
    "                sfc.append(ds[key_match])\n",
    "\n",
    "            elif 'prmsl' in list(ds.data_vars):\n",
    "                sfc.append(ds['prmsl'])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    sfc = xr.merge(sfc).drop('t')\n",
    "    iso = xr.merge(iso).rename({'isobaricInhPa':'level'})\n",
    "    iso = iso.sel(level=iso.level[::-1])\n",
    "\n",
    "    sfc['longitude'] = sfc['longitude'] - 360\n",
    "    iso['longitude'] = iso['longitude'] - 360\n",
    "    \n",
    "    return [sfc.drop(['surface', 'meanSea', 'step']), \n",
    "            iso.drop('step')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing (t) to t index level AGL\n",
      "Reducing (u) to u index level AGL\n",
      "Reducing (v) to v index level AGL\n",
      "Reducing (gh) to z index level AGL\n",
      "Reducing (r) to r index level AGL\n",
      "Reducing (w) to w index level AGL\n",
      "Reducing (absv) to vo index level AGL\n",
      "Reducing (dir) to dir index level AGL\n",
      "Reducing (spd) to spd index level AGL\n",
      "Reducing (u) to u index level AGL\n",
      "Reducing (v) to v index level AGL\n",
      "Reducing (u10) to u10m index level AGL\n",
      "Reducing (v10) to v10m index level AGL\n",
      "Reducing (t2m) to t2m index level AGL\n",
      "Reducing (d2m) to d2m index level AGL\n",
      "Reducing (u100) to u100m index level AGL\n",
      "Reducing (v100) to v100m index level AGL\n",
      "Reducing (prmsl) to msl index level AGL\n",
      "Reducing (cape) to cape index level AGL\n",
      "Reducing (sp) to sp index level AGL\n",
      "Reducing (tp) to swe_mm index level AGL\n",
      "Reducing (hpbl) to blh index level AGL\n",
      "Reducing (dir10m) to dir10m index level AGL\n",
      "Reducing (spd10m) to spd10m index level AGL\n",
      "Reducing (dir100m) to dir100m index level AGL\n",
      "Reducing (spd100m) to spd100m index level AGL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Timestamp('2017-01-26 00:00:00'), 13.1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfs_sample = xr.open_dataset('./gfs_latlon_grid.nc')\n",
    "gfs_sample['longitude'] = gfs_sample['longitude'] - 360\n",
    "gfs_lat, gfs_lon = gfs_sample['latitude'], gfs_sample['longitude']\n",
    "\n",
    "idx1d = (np.abs(gfs_lon - site_lon) + np.abs(gfs_lat - site_lat))\n",
    "lon_idx, lat_idx = np.where(idx1d == np.min(idx1d))\n",
    "lon_idx, lat_idx = lon_idx[0], lat_idx[0]\n",
    "\n",
    "init = valid - timedelta(hours=interval)\n",
    "\n",
    "f0, f1, fi = 24-interval+3, 24, 3\n",
    "fhrs = ['f%03d'%i for i in np.arange(f0, f1+1, fi)]\n",
    "\n",
    "flist = glob(archive + init.strftime(date_fmt) + \n",
    "             '/models/%s/*%s*.grib2'%(model, init.strftime(datetime_fmt)))[1:]\n",
    "\n",
    "flist = [f for f in flist if f.split('.')[-3] in fhrs]\n",
    "\n",
    "returns = [ingest_gfs(f) for f in flist]\n",
    "returns = np.array(returns, dtype=object)\n",
    "sfc, iso = returns[:, 0], returns[:, 1]\n",
    "    \n",
    "iso = xr.concat(iso, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')\n",
    "sfc = xr.concat(sfc, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')\n",
    "\n",
    "u, v = iso['u'], iso['v']\n",
    "wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "\n",
    "iso['dir'] = wdir\n",
    "iso['spd'] = np.sqrt(u**2 + v**2)\n",
    "\n",
    "for hgt in [10, 100]:\n",
    "    \n",
    "    u, v = sfc['u%d'%hgt], sfc['v%d'%hgt]\n",
    "    wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "    wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "    wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "    \n",
    "    sfc['dir%dm'%hgt] = wdir\n",
    "    sfc['spd%dm'%hgt] = np.sqrt(u**2 + v**2)\n",
    "    \n",
    "orog = sfc.orog\n",
    "gh = iso.gh\n",
    "\n",
    "lowest_level = np.full(orog.shape, fill_value=np.nan)\n",
    "lowest_level_index = np.full(orog.shape, fill_value=np.nan)\n",
    "\n",
    "for i, level in enumerate(iso['level']):\n",
    "    \n",
    "    lev_gh = gh.sel(level=level)\n",
    "    lowest_level = xr.where(orog >= lev_gh, level.values, lowest_level)\n",
    "    lowest_level_index = xr.where(orog >= lev_gh, i, lowest_level_index)\n",
    "    \n",
    "lowest_level_index = xr.where(np.isnan(lowest_level), 0, lowest_level_index)\n",
    "lowest_level = xr.where(np.isnan(lowest_level), 1000, lowest_level)\n",
    "\n",
    "df = []\n",
    "match_rename = {'absv':'vo', 'gh':'z', 'hpbl':'blh', 'prmsl':'msl', 'tp':'swe_mm',\n",
    "               'u10':'u10m', 'v10':'v10m', 'u100':'u100m', 'v100':'v100m'}\n",
    "\n",
    "# Loop over each variable in the xarray\n",
    "for ds in [iso, sfc.drop('orog')]:\n",
    "    \n",
    "    for var_name in ds.data_vars:\n",
    "        \n",
    "        new_var_name = match_rename[var_name] if var_name in match_rename.keys() else var_name\n",
    "        print('Reducing (%s) to %s index level AGL'%(var_name, new_var_name))\n",
    "\n",
    "        var = ds[var_name]\n",
    "\n",
    "        if 'level' in var.coords:\n",
    "\n",
    "            for i in np.arange(10):\n",
    "\n",
    "                var_agl = np.full(shape=(orog.shape), fill_value=np.nan)\n",
    "\n",
    "                for j, level in enumerate(iso['level']):\n",
    "\n",
    "                    var_agl = xr.where(lowest_level_index+i == j, var.isel(level=j), var_agl)\n",
    "\n",
    "                    # Record the levels used, should match lowest_level array, sanity check\n",
    "                    # var_agl[i, :, :] = xr.where(lowest_level_index+i == j, level, var_agl[i, :, :])\n",
    "\n",
    "                # We could ho ahead and append to the pandas dataframe here \n",
    "                # at the completion of each level (_01agl, _02agl...)\n",
    "                # We will have to use [(time), lat, lon] as a multiindex\n",
    "                var_agl = xr.DataArray(var_agl, \n",
    "                     dims=['time'], \n",
    "                     coords={'time':ds['time'],\n",
    "                             'latitude':ds['latitude'], \n",
    "                             'longitude':ds['longitude']})\n",
    "\n",
    "                df.append(var_agl.to_dataframe(name='%s_%02dagl'%(new_var_name.upper(), i+1)))\n",
    "\n",
    "                del var_agl\n",
    "                gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            var_agl = xr.DataArray(var.values, \n",
    "                dims=['time'], \n",
    "                coords={'time':ds['time'],\n",
    "                    'latitude':ds['latitude'], \n",
    "                     'longitude':ds['longitude']})\n",
    "\n",
    "            df.append(var_agl.to_dataframe(name='%s'%new_var_name.upper()))\n",
    "\n",
    "# SLOW!!! Is there anything we can do here??\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=['time', 'latitude', 'longitude']), df)\n",
    "df = df.rename(columns={'SWE_MM':'swe_mm'})\n",
    "\n",
    "scaler_file = glob(mlmodel_dir + '*scaler*')[-1]\n",
    "stats_file = glob(mlmodel_dir + '*train_stats*')[-1]\n",
    "model_file = glob(mlmodel_dir + '*SLRmodel*')[-1]\n",
    "\n",
    "if use_era_scaler == True:\n",
    "    with open(scaler_file, 'rb') as rfp:\n",
    "        scaler = pickle.load(rfp)\n",
    "else:\n",
    "    scaler = RobustScaler(quantile_range=(25, 75))\n",
    "\n",
    "with open(stats_file, 'rb') as rfp:\n",
    "    train_stats, train_stats_norm = pickle.load(rfp)\n",
    "    model_keys = train_stats.keys()\n",
    "    \n",
    "with open(model_file, 'rb') as rfp:\n",
    "    SLRmodel = pickle.load(rfp)\n",
    "    \n",
    "df = df.loc[:, model_keys]\n",
    "scaler = scaler.fit(df)\n",
    "\n",
    "df_norm = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.keys())\n",
    "\n",
    "slr = pd.DataFrame(SLRmodel.predict(df_norm), \n",
    "                   index=df_norm.index, columns=['slr']\n",
    "                  ).to_xarray()['slr']\n",
    "\n",
    "slr = xr.where(slr < 0, 0, slr)\n",
    "\n",
    "swe = df['swe_mm']\n",
    "snow = swe * slr\n",
    "slr_weighted = round(snow.sum()/swe.sum(), 1)\n",
    "\n",
    "valid = pd.to_datetime(slr[-1].time.values)\n",
    "valid, slr_weighted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
