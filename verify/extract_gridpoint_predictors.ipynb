{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "import pickle\n",
    "import cfgrib\n",
    "import pygrib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial, reduce\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grids(init):\n",
    "    \n",
    "    if init.day == 1:\n",
    "        print('\\rProgress: %s'%init.strftime('%Y%m%d'), end='')\n",
    "    \n",
    "    valid = init + timedelta(hours=delay+interval)\n",
    "    \n",
    "    fhr_step = 3\n",
    "    fhr_start = delay + fhr_step\n",
    "    fhr_end = delay + interval\n",
    "    fhrs = np.arange(fhr_start, fhr_end+fhr_step, fhr_step)\n",
    "    \n",
    "    init_dir = archive + '%s/models/gfs0p25/'%init.strftime('%Y%m%d')\n",
    "    \n",
    "    init_flist = sorted(np.hstack(\n",
    "        [glob(init_dir + '*%s*f%03d*.grib2'%(init.strftime('%Y%m%d%H'), fhr)) \n",
    "         for fhr in fhrs]))\n",
    "    \n",
    "    try:\n",
    "        returns = [ingest_gfs(f, grid_index=idx1d) for f in init_flist]\n",
    "        returns = np.array([r for r in returns if r is not None], dtype=object)\n",
    "        sfc, iso = returns[:, 0], returns[:, 1]\n",
    "\n",
    "        iso = xr.concat(iso, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')\n",
    "        sfc = xr.concat(sfc, dim='valid_time').drop('time').rename({'valid_time':'time'}).sortby('time')\n",
    "\n",
    "        iso['init'] = sfc['init'] = init\n",
    "        iso['valid'] = sfc['valid'] = valid\n",
    "\n",
    "        iso = iso.set_coords(['init', 'valid'])\n",
    "        sfc = sfc.set_coords(['init', 'valid'])\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return sfc, iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_gfs(f, grid_index):\n",
    "    \n",
    "    lon_idx, lat_idx = grid_index[0][0], grid_index[1][0] \n",
    "    \n",
    "    datasets = cfgrib.open_datasets(f)\n",
    "\n",
    "    keep_keys = ['tp', 'q', 't', 'u', 'v', 'absv', 'w', 'gh', 'r', 'd', \n",
    "                  'u10', 'v10', 'u100', 'v100', 't2m', 'd2m', \n",
    "                  'cape', 'prmsl', 'sp', 'orog', 'hpbl']\n",
    "\n",
    "    #keep_keys = ['tp', 't', 'u', 'v', 'gh', 'r', 'orog']\n",
    "\n",
    "    sfc, iso = [], []\n",
    "\n",
    "    for ds in datasets:\n",
    "        \n",
    "        ds = ds.isel(latitude=lat_idx, longitude=lon_idx).load()\n",
    "\n",
    "        key_match = np.array(list(ds.data_vars))[np.isin(list(ds.data_vars), keep_keys)]\n",
    "\n",
    "        if len(key_match) > 0:\n",
    "\n",
    "            dims = ds.dims.keys()\n",
    "            coords = ds[key_match].coords\n",
    "\n",
    "            if ('heightAboveGround' in coords) & ('heightAboveGround' not in dims):\n",
    "                sfc.append(ds[key_match].drop('heightAboveGround'))\n",
    "\n",
    "            elif 'isobaricInhPa' in coords:\n",
    "                iso.append(ds[key_match])\n",
    "\n",
    "            elif (('surface' in coords)|('meanSea' in coords)):\n",
    "                sfc.append(ds[key_match])\n",
    "\n",
    "            elif 'prmsl' in list(ds.data_vars):\n",
    "                sfc.append(ds['prmsl'])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        sfc = xr.merge(sfc).drop('t')\n",
    "        iso = xr.merge(iso).rename({'isobaricInhPa':'level'})\n",
    "        iso = iso.sel(level=iso.level[::-1])\n",
    "\n",
    "        sfc['longitude'] = sfc['longitude'] - 360\n",
    "        iso['longitude'] = iso['longitude'] - 360\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return [sfc.drop(['surface', 'meanSea', 'step']), \n",
    "                iso.drop('step')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site, interval, delay = 'CLNX', 12, 12\n",
    "site_lat, site_lon = 40.5763, -111.6383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gfs0p25'\n",
    "\n",
    "temp = '/scratch/general/lustre/u1070830/binary_temp/'; os.makedirs(temp, exist_ok=True)\n",
    "archive = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/archive/'\n",
    "\n",
    "date_fmt = '%Y%m%d'\n",
    "datetime_fmt = '%Y%m%d%H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = pd.date_range(\n",
    "    datetime(2015, 1, 15, 0, 0),\n",
    "    datetime(2019, 5, 31, 23, 59),\n",
    "    freq='12H')\n",
    "\n",
    "inits = [init for init in inits \n",
    "         if ((init.month <= 5) | (init.month >= 10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_sample = xr.open_dataset('./gfs_latlon_grid.nc')\n",
    "gfs_sample['longitude'] = gfs_sample['longitude'] - 360\n",
    "gfs_lat, gfs_lon = gfs_sample['latitude'], gfs_sample['longitude']\n",
    "\n",
    "idx1d = (np.abs(gfs_lon - site_lon) + np.abs(gfs_lat - site_lat))\n",
    "idx1d = np.where(idx1d == np.min(idx1d))\n",
    "\n",
    "print(gfs_sample.isel(latitude=idx1d[1], longitude=idx1d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.get_context('fork').Pool(256) as p:\n",
    "    returns = p.map(process_grids, inits, chunksize=1)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "returns = np.array([r for r in returns if r is not None], dtype=object)\n",
    "sfc = xr.concat(returns[:, 0], dim='time')\n",
    "iso = xr.concat(returns[:, 1], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = iso['u'], iso['v']\n",
    "wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "\n",
    "iso['dir'] = wdir\n",
    "iso['spd'] = np.sqrt(u**2 + v**2)\n",
    "\n",
    "for hgt in [10, 100]:\n",
    "\n",
    "    u, v = sfc['u%d'%hgt], sfc['v%d'%hgt]\n",
    "    wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "    wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "    wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "\n",
    "    sfc['dir%dm'%hgt] = wdir\n",
    "    sfc['spd%dm'%hgt] = np.sqrt(u**2 + v**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "match_rename = {'absv':'vo', 'gh':'z', 'hpbl':'blh', 'prmsl':'msl', 'tp':'swe_mm',\n",
    "               'u10':'u10m', 'v10':'v10m', 'u100':'u100m', 'v100':'v100m'}\n",
    "\n",
    "# Loop over each variable in the xarray\n",
    "for ds in [iso, sfc.drop('orog')]:\n",
    "\n",
    "    for var_name in ds.data_vars:\n",
    "\n",
    "        new_var_name = match_rename[var_name] if var_name in match_rename.keys() else var_name\n",
    "        # print('Reducing (%s) to %s index level AGL'%(var_name, new_var_name))\n",
    "\n",
    "        var = ds[var_name]\n",
    "        \n",
    "        if 'level' in var.coords:\n",
    "            for level in var.level:\n",
    "                if level >= 400:\n",
    "\n",
    "                    print('\\r%s %s'%(var_name, level.values), end='')\n",
    "\n",
    "                    var_agl = var.sel(level=level).drop(['init', 'valid', 'level', 'latitude', 'longitude'])\n",
    "                    df.append(var_agl.to_dataframe(name='%s_%d'%(new_var_name.upper(), level.values)))\n",
    "\n",
    "                    del var_agl\n",
    "                    gc.collect()\n",
    "        else:\n",
    "            df.append(var.drop(['init', 'valid', 'latitude', 'longitude']\n",
    "                              ).to_dataframe(name='%s'%new_var_name.upper()))\n",
    "            \n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=['time']), df)\n",
    "df = df.rename(columns={'SWE_MM':'swe_mm'})\n",
    "\n",
    "df.to_pickle('./%s_%dh_delay%d_extract_gfs_ISO.pd'%(site, interval, delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orog = sfc.orog\n",
    "gh = iso.gh\n",
    "\n",
    "lowest_level = np.full(orog.shape, fill_value=np.nan)\n",
    "lowest_level_index = np.full(orog.shape, fill_value=np.nan)\n",
    "\n",
    "for i, level in enumerate(iso['level'][::-1]):\n",
    "    \n",
    "    lev_gh = gh.sel(level=level)\n",
    "    \n",
    "    lowest_level = xr.where(orog >= lev_gh, level.values, lowest_level)\n",
    "    lowest_level_index = xr.where(orog >= lev_gh, i, lowest_level_index)\n",
    "\n",
    "lowest_level_index = xr.where(np.isnan(lowest_level), 0, lowest_level_index)\n",
    "lowest_level = xr.where(np.isnan(lowest_level), 1000, lowest_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force fix\n",
    "lowest_level = 800\n",
    "\n",
    "levels_above = iso.level[iso.level <= lowest_level][::-1][:10].values\n",
    "levels_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "match_rename = {'absv':'vo', 'gh':'z', 'hpbl':'blh', 'prmsl':'msl', 'tp':'swe_mm',\n",
    "               'u10':'u10m', 'v10':'v10m', 'u100':'u100m', 'v100':'v100m'}\n",
    "\n",
    "# Loop over each variable in the xarray\n",
    "for ds in [iso, sfc.drop('orog')]:\n",
    "\n",
    "    for var_name in ds.data_vars:\n",
    "\n",
    "        new_var_name = match_rename[var_name] if var_name in match_rename.keys() else var_name\n",
    "        # print('Reducing (%s) to %s index level AGL'%(var_name, new_var_name))\n",
    "\n",
    "        var = ds[var_name]\n",
    "\n",
    "        \n",
    "        if 'level' in var.coords:\n",
    "            \n",
    "            for i, level in enumerate(levels_above):\n",
    "                \n",
    "                var_agl = var.sel(level=level)\n",
    "\n",
    "                # We could go ahead and append to the pandas dataframe here \n",
    "                # at the completion of each level (_01agl, _02agl...)\n",
    "                # We will have to use [(time), lat, lon] as a multiindex\n",
    "                var_agl = xr.DataArray(var_agl[::-1], \n",
    "                     dims=['time'], \n",
    "                     coords={'time':ds['time'],\n",
    "                             'latitude':ds['latitude'], \n",
    "                             'longitude':ds['longitude']})\n",
    "                \n",
    "                df.append(var_agl.to_dataframe(name='%s_%02dagl'%(new_var_name.upper(), i+1)))\n",
    "\n",
    "                del var_agl\n",
    "                gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            var_agl = xr.DataArray(var.values, \n",
    "                dims=['time'], \n",
    "                coords={'time':ds['time'],\n",
    "                    'latitude':ds['latitude'], \n",
    "                     'longitude':ds['longitude']})\n",
    "\n",
    "            df.append(var_agl.to_dataframe(name='%s'%new_var_name.upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reduce(lambda left, right: pd.merge(left, right, on=['time', 'latitude', 'longitude']), df)\n",
    "df = df.rename(columns={'SWE_MM':'swe_mm'}).drop(columns=['latitude', 'longitude'])\n",
    "swe = df['swe_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./%s_%dh_delay%d_extract_gfs_LAG.pd'%(site, interval, delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
