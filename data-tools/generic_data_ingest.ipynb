{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from os.path import isfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SITE':{\n",
      "\t'header':[],\n",
      "\t'rename':[],\n",
      "\t'units':[],\n",
      "\t'tzinfo':None,\n",
      "\t'tfmt':['', ''],\n",
      "\t'tfix':None,\n",
      "\t'tshift':None,\n",
      "\t'auto_site':''\n",
      "},\n"
     ]
    }
   ],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'\n",
    "\n",
    "site_list = np.array([f.split('/')[-1].split('_')[0] for f in glob(obdir + '*.csv') if 'AUTO' not in f])\n",
    "site_list = np.unique(site_list[np.argsort(site_list)])\n",
    "\n",
    "# Dictionary builder (print blank entry)\n",
    "for s in ['SITE']:\n",
    "    print(\"'%s':{\\n\\t'header':[],\\n\\t'rename':[],\\n\\t'units':[],\\n\\t'tzinfo':None,\\n\\t'tfmt':['', ''],\\n\\t'tfix':None,\\n\\t'tshift':None,\\n\\t'auto_site':''\\n},\"%s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = {\n",
    "    'CLNX':{\n",
    "        'header':['DATE', 'SNOW', 'SWE', 'DEPTH24', 'SNOW24', 'SWE24'],\n",
    "        'rename':['datetime', 'snow12', 'swe12', 'depth24', 'snow24', 'swe24'],\n",
    "        'units':[None, 'in', 'in', 'in', 'in', 'in'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':['%Y%m%d%H%M'],\n",
    "        'tfix':None,\n",
    "        'tshift':None,\n",
    "        'auto_site':'CLN',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the manual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4b2edccc16f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Convert to UTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime_utc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tzinfo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m# Clean things up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datetime_utc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'I' is not defined"
     ]
    }
   ],
   "source": [
    "site = 'CLNX'\n",
    "\n",
    "finfo = file_info[site]\n",
    "\n",
    "f = glob(obdir + '%s*.csv'%site)[0]\n",
    "\n",
    "data_raw = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "for old, new, unit in zip(finfo['header'], finfo['rename'], finfo['units']):\n",
    "    if new is not None:\n",
    "        new = new+'_%s'%unit if unit is not None else new\n",
    "        data_raw = data_raw.rename(columns={old:new})\n",
    "        \n",
    "        if unit is not None:\n",
    "            data_raw[new] = data_raw[new].astype(np.float32)\n",
    "            \n",
    "    else:\n",
    "        data_raw = data_raw.drop(columns=old)\n",
    "                \n",
    "if finfo['tfix'] is not None:\n",
    "    data_raw['datetime'] = [datetime.strptime(t, finfo['tfmt'])+timedelta(hours=finfo['tfix']) for t in data_raw['date']]\n",
    "    \n",
    "elif len(finfo['tfmt']) == 2:\n",
    "    data_raw['time'] = data_raw['time'].astype(str)\n",
    "    tfmt = ' '.join(finfo['tfmt'])\n",
    "    data_raw['datetime'] = [datetime.strptime(' '.join(dt), tfmt) for dt in zip(data_raw['date'], data_raw['time'])]\n",
    "    data_raw['datetime'] = [d+timedelta(minutes=60-d.minute) \n",
    "                            if d.minute > 30 else d-timedelta(minutes=d.minute) for d in data_raw['datetime']]\n",
    "    \n",
    "elif len(finfo['tfmt']) == 1:\n",
    "    data_raw['datetime'] = [datetime.strptime(str(t), finfo['tfmt'][0]) for t in data_raw['datetime']]\n",
    "\n",
    "# If we are going to attempt to assign tzinfo and parse ST vs DT, do it here and now\n",
    "# For now this is timezone-naive\n",
    "# Convert to UTC\n",
    "data_raw['datetime_utc'] = data_raw['datetime'] + timedelta(hours=finfo['tzinfo'])\n",
    "I\n",
    "# Clean things up\n",
    "data_raw = data_raw.set_index('datetime_utc')\n",
    "data_raw = data_raw.sort_index()\n",
    "data_raw = data_raw.drop(columns=[k for k in data_raw.keys() if (('date' in k)|('time' in k))])\n",
    "    \n",
    "in_mm = 25.4\n",
    "cm_mm = 10\n",
    "m_mm = 1000\n",
    "\n",
    "for k in data_raw.keys():\n",
    "    \n",
    "    data_raw[k] = data_raw[k].replace(-999, np.nan)\n",
    "    \n",
    "    if k[-3:] == '_in':\n",
    "        data_raw[k] *= in_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_in', '_mm')}).round(2)\n",
    "    elif k[-3:] == '_cm':\n",
    "        data_raw[k] *= cm_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_cm', '_mm')}).round(2)\n",
    "    elif k[-2:] == '_m':\n",
    "        data_raw[k] *= m_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_m', '_mm')}).round(2)\n",
    "    elif k[-5:] == '_degF':\n",
    "        data_raw[k] = (data_raw[k] - 32) * (5 / 9)\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_degF', '_degC')}).round(2)\n",
    "        \n",
    "intervals = [k.split('_')[0].replace('snow', '') for k in data_raw.keys() if ('snow' in k)]\n",
    "for i in intervals:\n",
    "    data_raw.insert(0, 'slr'+i, np.round(data_raw['snow%s_mm'%i]/data_raw['swe%s_mm'%i], 2))\n",
    "    data_raw['slr'+i][np.isinf(data_raw['slr'+i])] = np.nan\n",
    "\n",
    "# Ensure data is typecast properly\n",
    "data_raw = data_raw.fillna(np.nan)\n",
    "for k in data_raw.keys():\n",
    "    try:\n",
    "        data_raw[k] = data_raw[k].astype(np.float32)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(data_raw.shape)\n",
    "data_raw = data_raw.dropna(how='all')\n",
    "print(data_raw.shape)\n",
    "\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the automated data as a baseline/sanity check\n",
    "Data is obtained from mesowest API - May use the timeseries or precipitation service<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/timeseries/<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/precipitation/<br>\n",
    "To save long records, will need to access as csv format not json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate precipitating (snow) periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the hourlies to match the 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the data isn't mislabeled by plotting/visualizing segments\n",
    "These should be correct, but check that the beginning of period isn't labeled instead of end, etc.<br>\n",
    "A time-shift correction can be applied if needed. Keep in mind the automated data will likely <br>\n",
    "underreport vs the manual observations so consider the timing more than event size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shift = finfo['tshift'] if finfo['tshift'] is not None else 0\n",
    "\n",
    "for i in [intervals[-1]]:\n",
    "    snowkey = 'snow%s_mm'%i\n",
    "    swekey = 'swe%s_mm'%i\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 12), facecolor='w')\n",
    "    \n",
    "    d0 = datetime(2018, 9, 1, 0, 0)\n",
    "    d1 = datetime(2019, 6, 1, 0, 0)\n",
    "    mask = ((data_all.index > d0) & (data_all.index <= d1) & \n",
    "           (data_all[swekey] > 0.) & (data_all[snowkey] > 0.))\n",
    "    \n",
    "    axs[0].set_title('%s SWE%s'%(site, i))\n",
    "\n",
    "    axs[0].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, swekey], color='C0')\n",
    "    axs[0].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, swekey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "    # axs[0].plot(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], color='C1')\n",
    "    # axs[0].scatter(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "    axs[1].set_title('%s SNOW%s'%(site, i))\n",
    "\n",
    "    axs[1].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey], color='C0')\n",
    "    axs[1].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "    # axs[1].plot(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], color='C1')\n",
    "    # axs[1].scatter(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "    axs[2].set_title('%s SLR%s'%(site, i))\n",
    "\n",
    "    axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey]/data_all.loc[mask, swekey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0', label='slr')\n",
    "    # axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_auto_mm']/data_all.loc[mask, 'swe_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1', label='slr_auto')\n",
    "\n",
    "    axs[2].set_ylim([0, 50])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlim([d0, d1])\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the time shift (if any) permanantly and save out (to netCDF or .pd?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save out the precip periods with the dataset (merge column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'precip_periods' not in data_all.keys():\n",
    "    data_all['precip_periods'] = np.full(data_all.shape[0], fill_value='[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save = data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_file = f.split('/')[-1].replace('.csv', '.pd')\n",
    "print('Saving %s'%data_save_file)\n",
    "\n",
    "data_save.to_pickle(obdir + 'clean/' + data_save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
