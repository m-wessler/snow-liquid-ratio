{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from os.path import isfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'\n",
    "\n",
    "site_list = np.array([f.split('/')[-1].split('_')[0] for f in glob(obdir + '*.csv') if 'AUTO' not in f])\n",
    "site_list = np.unique(site_list[np.argsort(site_list)])\n",
    "\n",
    "# Dictionary builder (print blank entry)\n",
    "for s in ['SITE']:\n",
    "    print(\"'%s':{\\n\\t'header':[],\\n\\t'rename':[],\\n\\t'units':[],\\n\\t'tzinfo':None,\\n\\t'tfmt':['', '']\\n\\t'tfix':None,\\n\\t'tshift':None,\\n\\t'auto_site':''\\n},\"%s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export this block to a module/config file later...\n",
    "\n",
    "# some standards... \n",
    "# precip/snow/depth should be swe12/snow12/depth12 or swe24/snow24/depth24 (or other interval)\n",
    "# temperature over an interval gt 1h should be noted as tmax/tavg/tmin (use t for instantaneous only)\n",
    "# manual does not need an identifier, auto should be swe12_auto/snow12_auto etc\n",
    "# tzinfo should be in STANDARD time, we can adjust for daylight time later if needed\n",
    "# tfmt should be split according to date columns - one column, one item; two columns, two items, etc\n",
    "# use tfix to set a fixed observation hour (local time HHMM)\n",
    "\n",
    "file_info = {\n",
    "    'ALTA':{\n",
    "        'header':['LOCATION','DATE','PRCP','SNOW','SNWD','TMAX','TMIN','TOBS'],\n",
    "        'rename':[None, 'date', 'swe24', 'snow24', 'depth24', 'tsfc_max', 'tsfc_min', 'tsfc'],\n",
    "        'units':[None, None, 'in', 'in', 'in', 'degF', 'degF', 'degF'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':'%m/%d/%Y',\n",
    "        'tfix':18,\n",
    "        'tshift':None,\n",
    "        'auto_site':'CLN',\n",
    "    },\n",
    "    'AGD':{\n",
    "        'header':['Location','DateMST','TimeMST','MaxT','MinT','12HrSnow','24HrSnow','12HrWater','24HrWater'],\n",
    "        'rename':[None, 'date', 'time', 'tsfc_max', 'tsfc_min', 'snow12', 'snow24', 'swe12', 'swe24'],\n",
    "        'units':[None, None, None, 'degF', 'degF', 'in', 'in', 'in', 'in'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':['%m/%d/%y', '%H:%M:%S'],\n",
    "        'tfix':None,\n",
    "        'tshift':None,\n",
    "        'auto_site':'CLN',\n",
    "    },\n",
    "    'BCC':{\n",
    "        'header':['Location','Date LOCAL','Time LOCAL','MaxT','MinT','12HrSnow','24HrSnow','12HrWater','24HrWater'],\n",
    "        'rename':[None, 'date', 'time', 'tsfc_max', 'tsfc_min', 'snow12', 'snow24', 'swe12', 'swe24'],\n",
    "        'units':[None, None, None, 'degF', 'degF', 'in', 'in', 'in', 'in'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':['%m/%d/%y', '%H:%M:%S'],\n",
    "        'tfix':None,\n",
    "        'tshift':None,\n",
    "        'auto_site':'BRC'\n",
    "    },\n",
    "    'SLB':{\n",
    "        'header':['NAME','DATE','PRCP','SNOW','SNWD','TMAX','TMIN','TOBS'],\n",
    "        'rename':[None, 'date', 'swe24', 'snow24', 'depth24', 'tsfc_max', 'tsfc_min', 'tsfc'],\n",
    "        'units':[None, None, 'in', 'in', 'in', 'degF', 'degF', 'degF'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':'%Y-%m-%d', #1917-08-22\n",
    "        'tfix':9,\n",
    "        'tshift':None,\n",
    "        'auto_site':'PKCU1' #or BRC\n",
    "    },\n",
    "    'PVC':{\n",
    "        'header':['Location','Date LOCAL','Time LOCAL','MaxT','MinT','12HrSnow','24HrSnow','12HrWater','24HrWater'],\n",
    "        'rename':[None, 'date', 'time', 'tsfc_max', 'tsfc_min', 'snow12', 'snow24', 'swe12', 'swe24'],\n",
    "        'units':[None, None, None, 'degF', 'degF', 'in', 'in', 'in', 'in'],\n",
    "        'tzinfo':-7,\n",
    "        'tfmt':['%m/%d/%y', '%H:%M:%S'],\n",
    "        'tfix':None,\n",
    "        'tshift':None,\n",
    "        'auto_site':None,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the manual observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'PVC'\n",
    "finfo = file_info[site]\n",
    "\n",
    "f = glob(obdir + '%s*.csv'%site)[0]\n",
    "\n",
    "data_raw = pd.read_csv(f, low_memory=False)\n",
    "\n",
    "for old, new, unit in zip(finfo['header'], finfo['rename'], finfo['units']):\n",
    "    if new is not None:\n",
    "        new = new+'_%s'%unit if unit is not None else new\n",
    "        data_raw = data_raw.rename(columns={old:new})\n",
    "        \n",
    "        if unit is not None:\n",
    "            data_raw[new] = data_raw[new].astype(np.float32)\n",
    "            \n",
    "    else:\n",
    "        data_raw = data_raw.drop(columns=old)\n",
    "        \n",
    "if finfo['tfix'] is not None:\n",
    "    data_raw['datetime'] = [datetime.strptime(t, finfo['tfmt'])+timedelta(hours=finfo['tfix']) for t in data_raw['date']]\n",
    "    \n",
    "elif len(finfo['tfmt']) == 2:\n",
    "    tfmt = ' '.join(finfo['tfmt'])\n",
    "    data_raw['datetime'] = [datetime.strptime(' '.join(dt), tfmt) for dt in zip(data_raw['date'], data_raw['time'])]\n",
    "    data_raw['datetime'] = [d+timedelta(minutes=60-d.minute) \n",
    "                            if d.minute > 30 else d-timedelta(minutes=d.minute) for d in data_raw['datetime']]\n",
    "    \n",
    "elif len(finfo['tfmt']) == 1:\n",
    "    pass\n",
    "    \n",
    "# If we are going to attempt to assign tzinfo and parse ST vs DT, do it here and now\n",
    "# For now this is timezone-naive\n",
    "# Convert to UTC\n",
    "data_raw['datetime_utc'] = data_raw['datetime'] + timedelta(hours=finfo['tzinfo'])\n",
    "\n",
    "# Clean things up\n",
    "data_raw = data_raw.set_index('datetime_utc')\n",
    "data_raw = data_raw.sort_index()\n",
    "data_raw = data_raw.drop(columns=[k for k in data_raw.keys() if (('date' in k)|('time' in k))])\n",
    "    \n",
    "in_mm = 25.4\n",
    "cm_mm = 1/10 \n",
    "m_mm = 1/1000\n",
    "\n",
    "for k in data_raw.keys():    \n",
    "    if k[-3:] == '_in':\n",
    "        data_raw[k] *= in_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_in', '_mm')}).round(2)\n",
    "    elif k[-3:] == '_cm':\n",
    "        data_raw[k] *= cm_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_cm', '_mm')}).round(2)\n",
    "    elif k[-2:] == '_m':\n",
    "        data_raw[k] *= m_mm\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_m', '_mm')}).round(2)\n",
    "    \n",
    "    elif k[-5:] == '_degF':\n",
    "        data_raw[k] = (data_raw[k] - 32) * (5 / 9)\n",
    "        data_raw = data_raw.rename(columns={k:k.replace('_degF', '_degC')}).round(2)\n",
    "        \n",
    "intervals = [k.split('_')[0].replace('snow', '') for k in data_raw.keys() if ('snow' in k)]\n",
    "for i in intervals:\n",
    "    data_raw.insert(0, 'slr'+i, data_raw['snow%s_mm'%i]/data_raw['swe%s_mm'%i])\n",
    "    data_raw['slr'+i][np.isinf(data_raw['slr'+i])] = np.nan\n",
    "\n",
    "# print(data_raw.shape)\n",
    "# data_raw = data_raw.drop_duplicates()\n",
    "# print(data_raw.shape)\n",
    "\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the automated data as a baseline/sanity check\n",
    "Data is obtained from mesowest API - May use the timeseries or precipitation service<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/timeseries/<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/precipitation/<br>\n",
    "To save long records, will need to access as csv format not json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate precipitating (snow) periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the hourlies to match the 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the data isn't mislabeled by plotting/visualizing segments\n",
    "These should be correct, but check that the beginning of period isn't labeled instead of end, etc.<br>\n",
    "A time-shift correction can be applied if needed. Keep in mind the automated data will likely <br>\n",
    "underreport vs the manual observations so consider the timing more than event size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shift = finfo['tshift'] if finfo['tshift'] is not None else 0\n",
    "\n",
    "for i in [intervals[-1]]:\n",
    "    snowkey = 'snow%s_mm'%i\n",
    "    swekey = 'swe%s_mm'%i\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 12), facecolor='w')\n",
    "    \n",
    "    d0 = datetime(2018, 9, 1, 0, 0)\n",
    "    d1 = datetime(2019, 6, 1, 0, 0)\n",
    "    mask = ((data_all.index > d0) & (data_all.index <= d1) & \n",
    "           (data_all[swekey] > 0.) & (data_all[snowkey] > 0.))\n",
    "    \n",
    "    axs[0].set_title('%s SWE%s'%(site, i))\n",
    "\n",
    "    axs[0].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, swekey], color='C0')\n",
    "    axs[0].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, swekey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "    # axs[0].plot(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], color='C1')\n",
    "    # axs[0].scatter(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "    axs[1].set_title('%s SNOW%s'%(site, i))\n",
    "\n",
    "    axs[1].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey], color='C0')\n",
    "    axs[1].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "    # axs[1].plot(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], color='C1')\n",
    "    # axs[1].scatter(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "    axs[2].set_title('%s SLR%s'%(site, i))\n",
    "\n",
    "    axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, snowkey]/data_all.loc[mask, swekey], \n",
    "                   marker='*', s=150, linewidth=0.5, color='C0', label='slr')\n",
    "    # axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_auto_mm']/data_all.loc[mask, 'swe_auto_mm'], \n",
    "    #                marker='*', s=150, linewidth=0.5, color='C1', label='slr_auto')\n",
    "\n",
    "    axs[2].set_ylim([0, 50])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlim([d0, d1])\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the time shift (if any) permanantly and save out (to netCDF or .pd?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save out the precip periods with the dataset (merge column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'precip_periods' not in data_all.keys():\n",
    "    data_all['precip_periods'] = np.full(data_all.shape[0], fill_value='[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save = data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_file = f.split('/')[-1].replace('.csv', '.pd')\n",
    "print('Saving %s'%data_save_file)\n",
    "\n",
    "data_save.to_pickle(obdir + 'clean/' + data_save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
