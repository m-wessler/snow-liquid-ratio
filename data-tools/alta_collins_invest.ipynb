{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from os.path import isfile\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the automated data as a baseline/sanity check\n",
    "Data is obtained from mesowest API - May use the timeseries or precipitation service<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/timeseries/<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/precipitation/<br>\n",
    "To save long records, will need to access as csv format not json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_auto = 'CLN_AUTO'\n",
    "f_auto = glob(obdir + '%s*.csv'%site_auto)[0]\n",
    "\n",
    "# Remove the units row (SI - C, mm)\n",
    "data_auto = pd.read_csv(f_auto, low_memory=False)[1:]\n",
    "data_auto = data_auto.drop(columns='Station_ID')\n",
    "\n",
    "renames_auto = {'Date_Time':'time', \n",
    "                'precip_accum_one_hour_set_1':'swe_auto_mm',\n",
    "                'snow_interval_set_1':'snow_auto_mm', \n",
    "                'snow_depth_set_1':'depth_auto_mm', \n",
    "                'air_temp_set_1':'tsfc_c'}\n",
    "\n",
    "data_auto = data_auto.rename(columns=renames_auto)\n",
    "\n",
    "# Fix the datatypes\n",
    "data_auto['time'] = data_auto['time'].astype('datetime64[ns]')\n",
    "data_auto = data_auto.set_index('time')\n",
    "\n",
    "data_auto = data_auto.astype({k:np.float32 for k in data_auto.keys()})\n",
    "\n",
    "# Noticed later in plotting that the data is labeled mm but acutally in cm, fix here\n",
    "data_auto['snow_auto_mm'] /= 10\n",
    "\n",
    "data_auto[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate precipitating (snow) periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the ERA 3h may be used, reduce hourly to 3h data\n",
    "kwargs = {'rule':'3H', 'base':0, \n",
    "          'label':'right', 'closed':'right'}\n",
    "\n",
    "data_auto3h = pd.DataFrame([\n",
    "    data_auto['tsfc_c'].resample(**kwargs).max(),\n",
    "    data_auto['swe_auto_mm'].resample(**kwargs).sum(),\n",
    "    data_auto['snow_auto_mm'].resample(**kwargs).sum()]).T\n",
    "\n",
    "data_auto3h[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these out to a file later\n",
    "precip_periods = data_auto[(\n",
    "    (data_auto['snow_auto_mm'] > 0.) & \n",
    "    (data_auto['swe_auto_mm'] > 0.) &\n",
    "    (data_auto['tsfc_c'] <= 6.))].index\n",
    "\n",
    "precip_periods3h = data_auto3h[(\n",
    "    (data_auto3h['snow_auto_mm'] > 0.) & \n",
    "    (data_auto3h['swe_auto_mm'] > 0.) &\n",
    "    (data_auto3h['tsfc_c'] <= 6.))].index\n",
    "\n",
    "precip_periods[:5], precip_periods3h[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'CLN'\n",
    "f = glob(obdir + '%s*.csv'%site)[0]\n",
    "\n",
    "data_raw = pd.read_csv(f, low_memory=False)\n",
    "data_raw = data_raw.set_index(['DATE'])\n",
    "\n",
    "data_time = np.array([[datetime.strptime(d+t, '%m/%d/%y %H%M') for t in [' 0400', ' 1600']] for d in data_raw.index]).flatten()\n",
    "data_raw = np.array([[(data_raw.loc[d]['%sWATER'%t], data_raw.loc[d]['%sSNOW'%t]) for t in ['0400', '1600']] for d in data_raw.index])\n",
    "data_raw = data_raw.reshape(-1, 2)\n",
    "\n",
    "data = pd.DataFrame([data_time, data_raw[:, 0], data_raw[:, 1]]).T\n",
    "data = data.rename(columns={0:'time', 1:'swe_in', 2:'snow_in'}).set_index('time')\n",
    "\n",
    "# Convert in to mm\n",
    "data['swe_mm'] = data['swe_in'] * 25.4\n",
    "data['snow_mm'] = data['snow_in'] * 25.4\n",
    "data['slr'] = data['snow_mm']/data['swe_mm']\n",
    "data = data.drop(columns=['swe_in', 'snow_in'])\n",
    "\n",
    "data = data.fillna(np.nan)\n",
    "for k in data.keys():\n",
    "    try:\n",
    "        data[k] = data[k].astype(np.float32)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the hourlies to match the 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auto_tmax12 = data_auto['tsfc_c'].resample('12H', closed='right', label='right', base=4).max().rename('tsfc_max_c')\n",
    "data_auto_swe12 = data_auto['swe_auto_mm'].resample('12H', closed='right', label='right', base=4).sum()\n",
    "data_auto_snow12 = data_auto['snow_auto_mm'].resample('12H', closed='right', label='right', base=4).sum()\n",
    "data_auto_12 = pd.DataFrame([data_auto_swe12, data_auto_snow12, data_auto_tmax12]).T\n",
    "data_auto_12[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data.merge(data_auto_12, on='time')\n",
    "data_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the data isn't mislabeled by plotting/visualizing segments\n",
    "These should be correct, but check that the beginning of period isn't labeled instead of end, etc.<br>\n",
    "A time-shift correction can be applied if needed. Keep in mind the automated data will likely <br>\n",
    "underreport vs the manual observations so consider the timing more than event size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the Alta dataset is labeled with the period START not END... \n",
    "# 12 hour time shift seems to fix the issue with the data\n",
    "time_shift = 12\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 12), facecolor='w')\n",
    "d0 = datetime(2018, 3, 1, 0, 0)\n",
    "d1 = datetime(2018, 4, 1, 0, 0)\n",
    "\n",
    "mask = ((data_all.index > d0) & (data_all.index <= d1) & \n",
    "        (data_all['swe_mm'] > 0.) & (data_all['snow_mm'] > 0.))\n",
    "\n",
    "axs[0].set_title('%s SWE'%site)\n",
    "\n",
    "axs[0].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'swe_mm'], color='C0')\n",
    "axs[0].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'swe_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "axs[0].plot(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], color='C1')\n",
    "axs[0].scatter(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "axs[1].set_title('%s SNOW'%site)\n",
    "\n",
    "axs[1].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm'], color='C0')\n",
    "axs[1].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "axs[1].plot(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], color='C1')\n",
    "axs[1].scatter(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "axs[2].set_title('%s SLR'%site)\n",
    "\n",
    "axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm']/data_all.loc[mask, 'swe_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0', label='slr')\n",
    "axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_auto_mm']/data_all.loc[mask, 'swe_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1', label='slr_auto')\n",
    "\n",
    "axs[2].set_ylim([0, 50])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim([d0, d1])\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the time shift permanantly and save out (to netCDF or .pd?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to time shift the MANUAL observations! Break out the values and shift the array as needed\n",
    "data_shift = data.copy(deep=True)\n",
    "data_shift.index = data_shift.index + timedelta(hours=time_shift)\n",
    "\n",
    "data_save = data_shift.merge(data_auto_12, on='time')\n",
    "\n",
    "data_save = data_save.rename(columns={'slr':'slr12', 'swe_mm':'swe12_mm', 'snow_mm':'snow12_mm', 'swe_auto_mm':'swe12_auto_mm', 'snow_auto_mm':'snow12_auto_mm',})\n",
    "\n",
    "data_save[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = []\n",
    "\n",
    "for t in data_save.index:\n",
    "    tidx = np.where( (precip_periods >= t-timedelta(hours=12)) & (precip_periods < t) )\n",
    "    periods.append([t, precip_periods[tidx].values])\n",
    "        \n",
    "periods = pd.DataFrame(periods, columns=['time', 'precip_periods']).set_index('time')\n",
    "data_save = data_save.merge(periods, on='time')\n",
    "data_save[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to UTC to time match with the ERA5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tzinfo = -7\n",
    "local_time = data_save.index\n",
    "utc_time = np.array([t+timedelta(hours=tzinfo) for t in data_save.index])\n",
    "data_save.index = utc_time\n",
    "\n",
    "data_save[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_file = f.split('/')[-1].replace('.csv', '.pd')\n",
    "print('Saving %s'%data_save_file)\n",
    "\n",
    "data_save.to_pickle(obdir + 'clean/' + data_save_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
