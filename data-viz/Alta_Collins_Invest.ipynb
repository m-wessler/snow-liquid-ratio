{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the automated data as a baseline/sanity check\n",
    "Data is obtained from mesowest API - May use the timeseries or precipitation service<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/timeseries/<br>\n",
    "https://developers.synopticdata.com/mesonet/v2/stations/precipitation/<br>\n",
    "To save long records, will need to access as csv format not json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_auto = 'CLN_AUTO'\n",
    "f_auto = glob(obdir + '%s*.csv'%site_auto)[0]\n",
    "\n",
    "# Remove the units row (SI - C, mm)\n",
    "data_auto = pd.read_csv(f_auto, low_memory=False)[1:]\n",
    "data_auto = data_auto.drop(columns='Station_ID')\n",
    "\n",
    "renames_auto = {'Date_Time':'time', \n",
    "                'precip_accum_one_hour_set_1':'swe_auto_mm',\n",
    "                'snow_interval_set_1':'snow_auto_mm', \n",
    "                'snow_depth_set_1':'depth_auto_mm', \n",
    "                'air_temp_set_1':'tsfc_c'}\n",
    "\n",
    "data_auto = data_auto.rename(columns=renames_auto)\n",
    "\n",
    "# Fix the datatypes\n",
    "data_auto['time'] = data_auto['time'].astype('datetime64[ns]')\n",
    "data_auto = data_auto.set_index('time')\n",
    "\n",
    "data_auto = data_auto.astype({k:np.float32 for k in data_auto.keys()})\n",
    "\n",
    "# Noticed later in plotting that the data is labeled mm but acutally in cm, fix here\n",
    "data_auto['snow_auto_mm'] /= 10\n",
    "\n",
    "data_auto[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate precipitating (snow) periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the ERA 3h may be used, reduce hourly to 3h data\n",
    "kwargs = {'rule':'3H', 'base':0, \n",
    "          'label':'right', 'closed':'right'}\n",
    "\n",
    "data_auto3h = pd.DataFrame([\n",
    "    data_auto['tsfc_c'].resample(**kwargs).max(),\n",
    "    data_auto['swe_auto_mm'].resample(**kwargs).sum(),\n",
    "    data_auto['snow_auto_mm'].resample(**kwargs).sum()]).T\n",
    "\n",
    "data_auto3h[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these out to a file later\n",
    "precip_periods = data_auto[(\n",
    "    (data_auto['snow_auto_mm'] > 0.) & \n",
    "    (data_auto['swe_auto_mm'] > 0.) &\n",
    "    (data_auto['tsfc_c'] <= 6.))].index\n",
    "\n",
    "precip_periods3h = data_auto3h[(\n",
    "    (data_auto3h['snow_auto_mm'] > 0.) & \n",
    "    (data_auto3h['swe_auto_mm'] > 0.) &\n",
    "    (data_auto3h['tsfc_c'] <= 6.))].index\n",
    "\n",
    "precip_periods[:5], precip_periods3h[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'CLN'\n",
    "f = glob(obdir + '%s*.csv'%site)[0]\n",
    "\n",
    "data_raw = pd.read_csv(f, low_memory=False)\n",
    "data_raw = data_raw.set_index(['DATE'])\n",
    "\n",
    "data_time = np.array([[datetime.strptime(d+t, '%m/%d/%y %H%M') for t in [' 0400', ' 1600']] for d in data_raw.index]).flatten()\n",
    "data_raw = np.array([[(data_raw.loc[d]['%sWATER'%t], data_raw.loc[d]['%sSNOW'%t]) for t in ['0400', '1600']] for d in data_raw.index])\n",
    "data_raw = data_raw.reshape(-1, 2)\n",
    "\n",
    "data = pd.DataFrame([data_time, data_raw[:, 0], data_raw[:, 1]]).T\n",
    "data = data.rename(columns={0:'time', 1:'swe_in', 2:'snow_in'}).set_index('time')\n",
    "\n",
    "# Convert in to mm\n",
    "data['swe_mm'] = data['swe_in'] * 25.4\n",
    "data['snow_mm'] = data['snow_in'] * 25.4\n",
    "data = data.drop(columns=['swe_in', 'snow_in'])\n",
    "\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the hourlies to match the 12h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auto_swe12 = data_auto['swe_auto_mm'].resample('12H', closed='right', label='right', base=4).sum()\n",
    "data_auto_snow12 = data_auto['snow_auto_mm'].resample('12H', closed='right', label='right', base=4).sum()\n",
    "data_auto_12 = pd.DataFrame([data_auto_swe12, data_auto_snow12]).T\n",
    "data_auto_12[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data.merge(data_auto_12, on='time')\n",
    "data_all[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure that the data isn't mislabeled by plotting/visualizing segments\n",
    "These should be correct, but check that the beginning of period isn't labeled instead of end, etc.<br>\n",
    "A time-shift correction can be applied if needed. Keep in mind the automated data will likely <br>\n",
    "underreport vs the manual observations so consider the timing more than event size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the Alta dataset is labeled with the period START not END... \n",
    "# 12 hour time shift seems to fix the issue with the data\n",
    "time_shift = 12\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 12), facecolor='w')\n",
    "d0 = datetime(2018, 3, 1, 0, 0)\n",
    "d1 = datetime(2018, 4, 1, 0, 0)\n",
    "\n",
    "mask = ((data_all.index > d0) & (data_all.index <= d1) & \n",
    "        (data_all['swe_mm'] > 0.) & (data_all['snow_mm'] > 0.))\n",
    "\n",
    "axs[0].set_title('%s SWE'%site)\n",
    "\n",
    "axs[0].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'swe_mm'], color='C0')\n",
    "axs[0].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'swe_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "axs[0].plot(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], color='C1')\n",
    "axs[0].scatter(data_all.loc[mask].index, data_all.loc[mask, 'swe_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "axs[1].set_title('%s SNOW'%site)\n",
    "\n",
    "axs[1].plot(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm'], color='C0')\n",
    "axs[1].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0')\n",
    "\n",
    "axs[1].plot(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], color='C1')\n",
    "axs[1].scatter(data_all.loc[mask].index, data_all.loc[mask, 'snow_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1')\n",
    "\n",
    "axs[2].set_title('%s SLR'%site)\n",
    "\n",
    "axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_mm']/data_all.loc[mask, 'swe_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C0', label='slr')\n",
    "axs[2].scatter(data_all.loc[mask].index + timedelta(hours=time_shift), data_all.loc[mask, 'snow_auto_mm']/data_all.loc[mask, 'swe_auto_mm'], \n",
    "               marker='*', s=150, linewidth=0.5, color='C1', label='slr_auto')\n",
    "\n",
    "axs[2].set_ylim([0, 50])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim([d0, d1])\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the time shift permanantly and save out (to netCDF or .pd?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to time shift the MANUAL observations! Break out the values and shift the array as needed\n",
    "data_shift = data.copy(deep=True)\n",
    "data_shift.index = data_shift.index + timedelta(hours=time_shift)\n",
    "\n",
    "data_save = data_shift.merge(data_auto_12, on='time')\n",
    "data_save[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the heavy lifting: extract a profile from the ERA5 and combine with the observation data\n",
    "This will allow us to have our predictor set and target variable in one dataset to train a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine ERA5 lat/lon gridpoint\n",
    "Import the metadata file and take what's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = glob(obdir + '*Metadata*.xlsx')[0]\n",
    "metadata = pd.read_excel(meta_file).set_index('code').loc[site]\n",
    "\n",
    "# Determine the start, end years from data if metadata is nan\n",
    "start, end = metadata['start'], metadata['end']\n",
    "start = data_save.index[0].year if np.isnan(start) else start\n",
    "start = data_save.index[0].year if data_save.index[0].year < start else start\n",
    "end = data_save.index[-1].year if np.isnan(end) else end\n",
    "end = data_save.index[-1].year if data_save.index[-1].year > end else end\n",
    "metadata['start'], metadata['end'] = start, end\n",
    "\n",
    "# Determine the lat, lon of the site from the metadata\n",
    "site_lat, site_lon = metadata['lat'], metadata['lon']\n",
    "site_elev = metadata['elevation_m']\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if a profile for this lat/lon exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the era5_orog file to check the lat/lon grid\n",
    "era5_orog_file = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/era5/era5_orog.nc'\n",
    "era5_orog = xr.open_dataset(era5_orog_file)['z'].isel(time=0)\n",
    "era5_orog = era5_orog.rename({'latitude':'lat', 'longitude':'lon'})\n",
    "era5_lat, era5_lon = era5_orog['lat'], era5_orog['lon']\n",
    "\n",
    "# Find the index of the correct lat lon\n",
    "idx1d = (np.abs(era5_lon - site_lon) + np.abs(era5_lat - site_lat))\n",
    "idx = np.unravel_index(np.argmin(idx1d, axis=None), idx1d.shape)\n",
    "\n",
    "# Subset and convert gpm to m\n",
    "era5_g = 9.80665\n",
    "era5_orog = era5_orog.isel(lat=idx[1], lon=idx[0])/era5_g\n",
    "\n",
    "era5_orog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string based on the lat and lon of the nearest era5 gridpoint\n",
    "era5_prof_file = 'era5prof_{}N_{}W.nc'.format(\n",
    "    era5_orog['lat'].values, abs(era5_orog['lon'].values))\n",
    "\n",
    "# if isfile(era5_prof_file):\n",
    "    # open profile\n",
    "    # check start, end \n",
    "    # if start, end outside of bounds, produce the missing years\n",
    "    # call the profile script with arguments for lat, lon, start, end\n",
    "    # concatenate the variables for new years, save output\n",
    "    # concatenate new years with old years, save output\n",
    "    \n",
    "# else:\n",
    "    # if profile is not found produce new\n",
    "    # call the profile script with arguments for lat, lon, start, end\n",
    "    # concatenate the variables, save output\n",
    "    \n",
    "cmd = ('source activate downscaled_slr; python ' + \n",
    "       '/uufs/chpc.utah.edu/common/home/u1070830/code/model-tools/era5/' + \n",
    "       'extract_profile.py {} {} {} {}'.format(\n",
    "            era5_orog['lat'].values, era5_orog['lon'].values, start, end))\n",
    "\n",
    "# Notebook\n",
    "# p = subprocess.Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "# output, err = p.communicate()\n",
    "\n",
    "# Scripted\n",
    "subprocess.run(cmd, shell=True, stderr=sys.stderr, stdout=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task List:\n",
    "#### Import profile data in the aggregate\n",
    "#### Export the compiled data as netCDF\n",
    "#### Now all training gridpoints/sites will be in a consistent format and easy to aggregate!\n",
    "We can either expand the level variables here and calculate derived variables now or in a later script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
