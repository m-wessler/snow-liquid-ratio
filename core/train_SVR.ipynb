{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which sites to train the SVR model on\n",
    "Can be all available, or a defined list 'site_list' passed, or single station as list e.g. ['CLN']<br>\n",
    "Use all with an exclude list is another alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob(obdir + 'combined/*.pd')\n",
    "\n",
    "# This can be a manual site list if desired\n",
    "site_list = np.unique([f.split('/')[-1].split('_')[0] for f in flist])\n",
    "\n",
    "# site_list = [s for s in site_list if 'BSNF' in s]\n",
    "# site_list = ['CLN', 'AGD', 'ALTA']\n",
    "\n",
    "site_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each site, choose which files to use\n",
    "For now, we will only use one set of observations from each site so samples remain independent<br>\n",
    "We can change this behavior down the road to use all or some intervals<br>\n",
    "Favor 'short' for the shortest interval available e.g. 6h, 'long' for longest interval e.g. 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# favor = 'long' #'long'\n",
    "\n",
    "flist = []\n",
    "for site in site_list:\n",
    "    \n",
    "    site_files = glob(obdir + 'combined/%s*.pd'%site)\n",
    "    \n",
    "#     if len(site_files) > 1:\n",
    "#         if favor == 'short':\n",
    "#             flist.append(\n",
    "#                 site_files[np.argmin([int(f.split('.')[-2].replace('h', '')) \n",
    "#                     for f in site_files])])\n",
    "            \n",
    "#         elif favor == 'long':\n",
    "#             np.argmax([int(f.split('.')[-2].replace('h', '')) for f in site_files])\n",
    "    \n",
    "#     else:\n",
    "    flist.append(site_files)#[0])\n",
    "        \n",
    "flist = np.hstack(flist)\n",
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish which model variables and levels to use\n",
    "This is entirely based on the model that the SVR will be applied to - maximize the predictor set<br>\n",
    "Helpful to open a sample GFS/NAM/HRRR grid or profile to derive the variable list from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the compiled data\n",
    "Prune unused variables on import based on the list above<br>\n",
    "Sub-surface levels have already been dealt with in the compilation script - modify methods there<br>\n",
    "Note that 12 vs 24 hour intervals have also been dealt with above, if both desired... write that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_var_type = ['mean']#, 'max', 'min']\n",
    "\n",
    "data = []\n",
    "for f in flist:\n",
    "    \n",
    "    site = f.split('/')[-1].split('_')[0]\n",
    "    interval = int(f.split('/')[-1].split('.')[-2].replace('h', ''))\n",
    "    \n",
    "    df = pd.read_pickle(f)\n",
    "    \n",
    "    exclude_keys = ['VO', 'Z', 'W'] #['Z', '2T', 'TSFC', 'CAPE', 'VO', 'W', 'V', 'U']\n",
    "    exclude_keys = np.array([[k if ex in k[:len(ex)] else np.nan \n",
    "                              for ex in exclude_keys] for k in df.keys()])\n",
    "    exclude_keys = exclude_keys.flatten()\n",
    "    exclude_keys = exclude_keys[exclude_keys != 'nan']\n",
    "    \n",
    "    keys = ['slr', 'swe_mm']\n",
    "\n",
    "    keys.extend(np.hstack([[k for k in df.keys() if vt in k] for vt in use_var_type]))\n",
    "\n",
    "    df = df.loc[:, keys].rename(columns={[k for k in keys if 'swe' in k][0]:'swe_mm'})\n",
    "    df = df.loc[:, :].rename(columns={[k for k in keys if 'swe' in k][0]:'swe_mm'})\n",
    "    df = df.rename(columns={[k for k in keys if 'slr' in k][0]:'slr'})\n",
    "    df = df.drop(columns=[k for k in keys if 'auto' in k])\n",
    "    \n",
    "    # df.insert(0, 'site', np.full(df.index.size, fill_value=site, dtype='U10'))\n",
    "    doy = [int(pd.to_datetime(d).strftime('%j')) for d in df.index]\n",
    "    df.insert(2, 'day_of_year', doy)\n",
    "    \n",
    "    data.append(df.reset_index().drop(columns='time'))\n",
    "\n",
    "data = pd.concat(data, sort=False)\n",
    "\n",
    "# Treat the mean value as the instantaneous value for later applications,\n",
    "# we can change this behavior later on if desired. \n",
    "# An alternate method would be to keep the 'mean' tag through training \n",
    "# and choose behavior upon application\n",
    "data = data.rename(columns={k:k.replace('_mean', '') for k in data.keys()})\n",
    "\n",
    "data = data.drop(columns='SP')\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard trim data based on criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_slr, max_slr = 2.5, 30\n",
    "max_T650 = 0 + 273.15\n",
    "min_swe_mm = 2.54\n",
    "\n",
    "data = data[data['slr'] >= min_slr]\n",
    "data = data[data['slr'] <= max_slr]\n",
    "data = data[data['T_01agl'] <= max_T650]\n",
    "data = data[data['swe_mm'] >= min_swe_mm]\n",
    "\n",
    "# data = data[[k for k in data.keys() if 'swe' not in k]]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(facecolor='w', figsize=(6, 6))\n",
    "data['slr'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate basic bulk statistics and linear correlations\n",
    "Quanitfy how certain variables are related and produce plots to visualize this<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removed: Normalize/Standardize\n",
    "Moved down to where datasets are split. If need be we can normalize for the plots below here too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix, R2 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Correlation Verical Plots, R2 Vertical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify autocorrelated and other dependent predictors\n",
    "Mask these if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test/Validate Sets\n",
    "If we want to expand the hyperparameter tuning, we can test for sensitivity to random sample<br>\n",
    "by looping from bottom of code block back to here<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(slr) for stratification (> 1 ct per class label)\n",
    "data = data.dropna()\n",
    "fac = 5\n",
    "slr = np.round(data['slr']/fac, 0)*fac\n",
    "slr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total: %d'%len(data))\n",
    "\n",
    "# Split into train/test sets\n",
    "train_size, test_size, random_state = None, 0.33, 5\n",
    "X_train, X_test = train_test_split(data, \n",
    "                                       test_size=test_size, train_size=train_size, \n",
    "                                       random_state=random_state, stratify=slr)\n",
    "\n",
    "# Perform a secondary split if separate validation set required\n",
    "\n",
    "# Split off the target variable now that TTsplit is done\n",
    "y_train, y_test = X_train.pop('slr'), X_test.pop('slr')\n",
    "# y_train = np.round(y_train/fac, 0)*fac\n",
    "\n",
    "print('Train: {}\\nTest: {}\\nValidate: {}'.format(X_train.shape[0], X_test.shape[0], None))\n",
    "\n",
    "train_stats = X_train.describe().T\n",
    "train_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize/Standardize the data\n",
    "We are using z-score normalization for now but other methods exist<br>\n",
    "This is an absolute must for working with any of the ML models available to us<br>\n",
    "Normalize based on the TRAINING dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(quantile_range=(25, 75)).fit(X_train)\n",
    "\n",
    "X_train_norm = pd.DataFrame(scaler.transform(X_train.loc[:, list(X_train.keys())]), columns=X_train.keys())\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test.loc[:, list(X_train.keys())]), columns=X_train.keys())\n",
    "\n",
    "X_train_norm.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out plots describing the variability in the data (+ before/after normalization?)<br>\n",
    "It would be valuable to do a similar set of/novel comparison plots with the ERA5 vs GFS<br>\n",
    "for each variable/level in the validation script (separate from and following this script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF/PCA Reduction of highly dimensional data\n",
    "There are implementations of atmospheric data in ML models that heavily leverage<br>\n",
    "EOFs to reduce highly dimensional data to simpler components<br>\n",
    "Evaluate if this is viable here as it will be applied to raw variables later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LASSO/Ridge regression to determine predictor rank/impact\n",
    "L1/L2 Parameter Tuning -- Use to select relevant predictors, reduce irrelevant to zero, rank by influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train_norm, y_train)\n",
    "\n",
    "# lr_train_score = lr.score(X_train_norm, y_train)\n",
    "# lr_test_score = lr.score(X_test_norm, y_test)\n",
    "\n",
    "# print('Linear Regression Train Score: %.3f'%lr_train_score)\n",
    "# print('Linear Regression Test Score: %.3f'%lr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(lr.coef_))[::-1]\n",
    "# lr_coefs = lr.coef_[ranksort]\n",
    "# lr_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = lr_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(lr_keys[mask], lr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(lr_keys[mask]):\n",
    "#     if lr_coefs[i] != 0:\n",
    "#         ax.text(lr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Linear Regression Coefs')\n",
    "# ax.set_xlabel('Linear Coef Value')\n",
    "# ax.grid(zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "\n",
    "# # The higher the alpha value, more restriction on the coefficients; \n",
    "# # low alpha > more generalization, coefficients are barely restricted\n",
    "# rr_alpha_tune = []\n",
    "# for alpha in np.append(np.arange(0.01, 1, 0.01), np.arange(1, 1000, 1)):\n",
    "\n",
    "#     rr = Ridge(alpha=alpha)\n",
    "#     rr.fit(X_train, y_train)\n",
    "\n",
    "#     rr_train_score = rr.score(X_train, y_train)\n",
    "#     rr_test_score = rr.score(X_test, y_test)\n",
    "    \n",
    "#     rr_alpha_tune.append([alpha, rr_train_score, rr_test_score, rr])\n",
    "# rr_alpha_tune = np.array(rr_alpha_tune)\n",
    "\n",
    "# alpha, rr_train_score, rr_test_score, rr = rr_alpha_tune[np.argmax(rr_alpha_tune[:, 2])]\n",
    "# print('Ridge Regression alpha: %.3f'%alpha)\n",
    "# print('Ridge Regression Train Score: %.3f'%rr_train_score)\n",
    "# print('Ridge Regression Test Score: %.3f'%rr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(rr.coef_))[::-1]\n",
    "# rr_coefs = rr.coef_[ranksort]\n",
    "# rr_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = rr_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(rr_keys[mask], rr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(rr_keys[mask]):\n",
    "#     if rr_coefs[i] != 0:\n",
    "#         ax.text(rr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Ridge Regression Coefs')\n",
    "# ax.set_xlabel('Ridge Coef Value')\n",
    "# ax.grid(zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# lasso_alpha_tune = []\n",
    "# for alpha in np.arange(0.001, .01, 0.001):\n",
    "\n",
    "#     lasso = Lasso(alpha=alpha, max_iter=10e5)\n",
    "#     lasso.fit(X_train_norm, y_train)\n",
    "\n",
    "#     lasso_train_score = lasso.score(X_train_norm, y_train)\n",
    "#     lasso_test_score = lasso.score(X_test_norm, y_test)\n",
    "#     lasso_coeff_used = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "#     lasso_alpha_tune.append([alpha, lasso_train_score, lasso_test_score, lasso])\n",
    "# lasso_alpha_tune = np.array(lasso_alpha_tune)\n",
    "\n",
    "# alpha, lasso_train_score, lasso_test_score, lasso = lasso_alpha_tune[np.argmax(lasso_alpha_tune[:, 2])]\n",
    "# print('Lasso alpha: %.3f'%alpha)\n",
    "# print('Lasso Train Score: %.3f'%lasso_train_score)\n",
    "# print('Lasso Test Score: %.3f'%lasso_test_score)\n",
    "# print('Number of Features Used: %d'%lasso_coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(lasso.coef_))[::-1]\n",
    "# lasso_coefs = lasso.coef_[ranksort]\n",
    "# lasso_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = lasso_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(lasso_keys[mask], lasso_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(lasso_keys[mask]):\n",
    "#     if lasso_coefs[i] != 0:\n",
    "#         ax.text(lasso_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Lasso Regression Coefs')\n",
    "# ax.set_xlabel('Lasso Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# # Grid-search for best parameters? (alpha and l1_ratio)\n",
    "\n",
    "# elastic = ElasticNet(alpha=0.009, l1_ratio=0.1, max_iter=10e5)\n",
    "# elastic.fit(X_train_norm, y_train)\n",
    "\n",
    "# elastic_train_score = elastic.score(X_train_norm, y_train)\n",
    "# elastic_test_score = elastic.score(X_test_norm, y_test)\n",
    "# elastic_coeff_used = np.sum(elastic.coef_ != 0)\n",
    "    \n",
    "# print('Elastic Train Score: %.3f'%elastic_train_score)\n",
    "# print('Elastic Test Score: %.3f'%elastic_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(elastic.coef_))[::-1]\n",
    "# elastic_coefs = elastic.coef_[ranksort]\n",
    "# elastic_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = elastic_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(elastic_keys[mask], elastic_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(elastic_keys[mask]):\n",
    "#     if elastic_coefs[i] != 0:\n",
    "#         ax.text(elastic_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Elastic Regression Coefs')\n",
    "# ax.set_xlabel('Elastic Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to use feature selection, we can do so here.\n",
    "Can choose the model from which to take features, do so manually, or not at all (set equal to X_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Top Predictor: %s'%elastic_keys[0])\n",
    "# fig, axs = plt.subplots(3, 3, figsize=(18, 18), facecolor='w')\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# for i, ax in enumerate(axs):\n",
    "#     ik, ic = elastic_keys[i], elastic_coefs[i]\n",
    "#     ax.scatter(X_train_norm[ik], y_train, c='k', marker='+', s=65, linewidth=0.5)\n",
    "#     ax.set_title('(%d: %.2f) %s'%(i+1, ic, ik))\n",
    "#     ax.grid()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_low_import = False\n",
    "\n",
    "# if drop_low_import:\n",
    "#     feature_selection = elastic_keys[mask]\n",
    "#     feature_selection\n",
    "\n",
    "#     pre_select = X_train_norm.keys()\n",
    "#     X_train_norm = X_train_norm[feature_selection]\n",
    "#     X_test_norm = X_test_norm[feature_selection]\n",
    "#     post_select = X_train_norm.keys()\n",
    "\n",
    "#     print('dropped:', [k for k in pre_select if k not in post_select])\n",
    "\n",
    "#     print(X_train_norm.shape)\n",
    "#     print(X_test_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# for layer_sizes in [500]:\n",
    "#     mlp = MLPRegressor(\n",
    "#         hidden_layer_sizes=(layer_sizes,), \n",
    "#         activation='relu', \n",
    "#         solver='sgd', \n",
    "#         alpha=0.00001, \n",
    "#         batch_size='auto', \n",
    "#         learning_rate='adaptive', \n",
    "#         learning_rate_init=0.01, \n",
    "#         power_t=0.5, \n",
    "#         max_iter=50000, \n",
    "#         shuffle=True, \n",
    "#         random_state=random_state, \n",
    "#         tol=0.0001, \n",
    "#         verbose=False, \n",
    "#         warm_start=False, \n",
    "#         momentum=0.7, \n",
    "#         nesterovs_momentum=True, \n",
    "#         early_stopping=False, \n",
    "#         validation_fraction=0.3, \n",
    "#         beta_1=0.5, \n",
    "#         beta_2=0.999, \n",
    "#         epsilon=1e-8)\n",
    "\n",
    "#     mlp.fit(X_train_norm, y_train)\n",
    "\n",
    "#     mlp_train_score = mlp.score(X_train_norm, y_train)\n",
    "#     mlp_test_score = mlp.score(X_test_norm, y_test)\n",
    "    \n",
    "#     # mlp_coeff_used = np.sum(mlp.coef_ != 0)\n",
    "\n",
    "#     print('Layer Sizes: %d'%layer_sizes)\n",
    "#     print('MLP Train Score: %.3f'%mlp_train_score)\n",
    "#     print('MLP Test Score: %.3f'%mlp_test_score)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = mlp.predict(X_test_norm)\n",
    "# mlp_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "# maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "# maxslr += 5\n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "# ax = axs[0]\n",
    "# maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "# ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "# ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "# ax.legend()\n",
    "\n",
    "# ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "# ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "# ax.grid()\n",
    "\n",
    "# ax = axs[1]\n",
    "# maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "# maxslr += 5\n",
    "# ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "# ax.set_xlabel('Observed SLR')\n",
    "# ax.set_ylabel('Predicted SLR')\n",
    "# ax.plot([0, maxslr], [0, maxslr])\n",
    "# ax.set_xlim([0, maxslr])\n",
    "# ax.set_ylim([0, maxslr])\n",
    "# ax.set_aspect('equal')\n",
    "# ax.grid()\n",
    "# axs[1].set_title('R2: %.3f\\nMAE: %.3f'%(mlp_test_score, mlp_mae))\n",
    "\n",
    "# ax = axs[2]\n",
    "# error = test_predictions - y_test\n",
    "# ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "# ax.set_xlabel('Prediction Error')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.grid()\n",
    "\n",
    "# fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Support Vector Regression Model\n",
    "(Other ML models like ANN from keras, tensorflow, pytorch may be used in this block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a K-Fold Cross Validation to assess model performance (Optional, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the SVR model multiprocess friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def SVR_mp(_params):\n",
    "    \n",
    "    print('.', end='')\n",
    "    \n",
    "    _i, _j, _C, _e = _params\n",
    "    \n",
    "    _model = SVR(\n",
    "                C=_C, #Ridge regularization parameter for (L2)^2 penalty\n",
    "                epsilon=_e, #Specifies the epsilon-tube within which no penalty is associated in the training loss function\n",
    "                kernel='rbf', #'linear', 'polynomial', 'rbf'\n",
    "                degree=3, #pass interger for 'polynomial' kernel, ignored otherwise\n",
    "                tol=0.001, #stopping tolerance\n",
    "                shrinking=False, \n",
    "                cache_size=200, \n",
    "                verbose=False)\n",
    "    \n",
    "    _model.fit(X_train_norm, y_train)\n",
    "    \n",
    "    test_predictions = _model.predict(X_test_norm).flatten()\n",
    "    _r2 = _model.score(X_test_norm, y_test) #sklearn.metrics.r2_score(y_test.values.flatten(), test_predictions)\n",
    "    _mse = sklearn.metrics.mean_squared_error(y_test.values.flatten(), test_predictions)\n",
    "    _mae = sklearn.metrics.mean_absolute_error(y_test.values.flatten(), test_predictions)\n",
    "    _mare = MARE(y_test.values.flatten(), test_predictions)\n",
    "    \n",
    "    return (_i, _j, _C, _e, _r2, _mae, _mse, _mare, _model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from sklearn.svm import SVR\n",
    "import sklearn\n",
    "\n",
    "# Best test\n",
    "crange = np.arange(0, 100, 5)\n",
    "erange = np.arange(0.0, 5.1, .25)\n",
    "\n",
    "# crange = np.arange(1, 25, 1)\n",
    "# erange = np.arange(0.1, 3.1, .1)\n",
    "\n",
    "params = {}\n",
    "params['r2'] = np.zeros((len(crange), len(erange)))\n",
    "params['mae'] = np.zeros((len(crange), len(erange)))\n",
    "params['mse'] = np.zeros((len(crange), len(erange)))\n",
    "params['mare'] = np.zeros((len(crange), len(erange)))\n",
    "params['model'] = np.empty((len(crange), len(erange)), dtype='object')\n",
    "params['epsilon'] = np.zeros((len(crange), len(erange)))\n",
    "params['C'] = np.zeros((len(crange), len(erange)))\n",
    "\n",
    "mp_params = np.array([[(i, j, C, e) for j, e in enumerate(erange)] \n",
    "                      for i, C in enumerate(crange)]).reshape(-1, 4)\n",
    "\n",
    "print('Iterations to attempt: %d'%len(mp_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVR, collect output iterated over hyperparameters (C, epsilon)<br>\n",
    "Specify the cost function/loss function used, tolerances, kernel, error metric, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paralellize here\n",
    "# mp_returns = [SVR_mp(_param) for _param in mp_params[:10]]\n",
    "\n",
    "with mp.get_context('fork').Pool(128) as p:\n",
    "    mp_returns = p.map(SVR_mp, mp_params, chunksize=1)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "for item in mp_returns:\n",
    "    \n",
    "    i, j, C, e, r2, mae, mse, mare, model = item\n",
    "    i, j = int(i), int(j)\n",
    "        \n",
    "    params['r2'][i, j] = r2\n",
    "    params['mse'][i, j] = mse\n",
    "    params['mae'][i, j] = mae\n",
    "    params['mare'][i, j] = mare\n",
    "    params['model'][i, j] = model\n",
    "    params['epsilon'][i, j] = e\n",
    "    params['C'][i, j] = C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model performance over time, cost/loss function evolution and skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot SVR output with hyperparameters (C, epsilon)<br>\n",
    "Apply a grid-search method to select best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_on, indexer, _ = 'R2', np.where(params['r2'] == params['r2'].max()), params['r2'].max()\n",
    "min_on, indexer, _ = 'MAE', np.where(params['mae'] == params['mae'].min()), params['mae'].min()\n",
    "min_on, indexer, _ = 'MSE', np.where(params['mse'] == params['mse'].min()), params['mse'].min()\n",
    "min_on, indexer, _ = 'MARE', np.where(params['mare'] == params['mare'].min()), params['mare'].min()\n",
    "\n",
    "for min_on in ['r2']:\n",
    "    \n",
    "    if min_on in ['mse', 'mae', 'mare']:\n",
    "        min_max = 'Minimized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].min())\n",
    "    elif min_on in ['r2']:\n",
    "        min_max = 'Maximized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].max())\n",
    "\n",
    "    r, c = indexer\n",
    "    r, c = r[0], c[0]\n",
    "    r, c, _\n",
    "\n",
    "    model = params['model'][r, c]\n",
    "    test_predictions = model.predict(X_test_norm)\n",
    "    \n",
    "    y_true = y_test\n",
    "    y_pred = test_predictions\n",
    "    print('MARE ', MARE(y_true, y_pred))\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, facecolor='w', figsize=(24, 14))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    ax = axs[0]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mae'])\n",
    "    plt.colorbar(cbar, label='mae', ax=ax)\n",
    "    ax.set_title('Min MAE: %.3f'%params['mae'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[1]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mse'])\n",
    "    plt.colorbar(cbar, label='mse', ax=ax)\n",
    "    ax.set_title('Min MSE: %.3f'%params['mse'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[2]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['r2'])\n",
    "    plt.colorbar(cbar, label='r2', ax=ax)\n",
    "    ax.set_title('Max R^2: %.3f'%params['r2'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='k', marker='+')\n",
    "\n",
    "    for ax in axs[:3]:\n",
    "        ax.set_xlabel('epsilon')\n",
    "        ax.set_ylabel('C_val')\n",
    "        ax.set_ylim([crange.min(), crange.max()])\n",
    "        ax.set_xlim([erange.min(), erange.max()])\n",
    "\n",
    "    ax = axs[3]\n",
    "    maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "    ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "    ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "    ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[4]\n",
    "    maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "    maxslr += 5\n",
    "    ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "    ax.set_xlabel('Observed SLR')\n",
    "    ax.set_ylabel('Predicted SLR')\n",
    "    ax.plot([0, maxslr], [0, maxslr])\n",
    "    ax.set_xlim([0, maxslr])\n",
    "    ax.set_ylim([0, maxslr])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[5]\n",
    "    error = test_predictions - y_test\n",
    "    ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "    ax.set_xlabel('Prediction Error')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid()\n",
    "\n",
    "    plt.suptitle('Support Vector Regression Model\\n%s\\n%s on: %s\\n\\nepsilon %.3f\\nc_val: %.3f'%(site_list, min_max, min_on.upper(), params['epsilon'][r, c], params['C'][r, c]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe final trained SVR model\n",
    "Descriptive plots of predictor coefficient rank/influence<br>\n",
    "Print out model parameters, error metrics, skill scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranksort = np.argsort(abs(model.coef_))[::-1]\n",
    "# svr_coefs = model.coef_[0][ranksort][0]\n",
    "# svr_keys = X_train_norm.keys()[ranksort][0]\n",
    "# mask = svr_coefs != 0\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(svr_keys[mask], svr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(svr_keys):\n",
    "#     if svr_coefs[i] != 0:\n",
    "#         ax.text(svr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# # ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('SVR Coefs')\n",
    "# ax.set_xlabel('SVR Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out a usable model along with the input parameters for application\n",
    "Save with a descriptive filename and a metadata text file!<br>\n",
    "This will make swapping out models for evaluation much simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
