{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which sites to train the SVR model on\n",
    "Can be all available, or a defined list 'site_list' passed, or single station as list e.g. ['CLN']<br>\n",
    "Use all with an exclude list is another alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob(obdir + 'combined/*.pd')\n",
    "\n",
    "# This can be a manual site list if desired\n",
    "site_list = np.unique([f.split('/')[-1].split('_')[0] for f in flist])\n",
    "# site_list = ['CLN']\n",
    "\n",
    "site_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each site, choose which files to use\n",
    "For now, we will only use one set of observations from each site so samples remain independent<br>\n",
    "We can change this behavior down the road to use all or some intervals<br>\n",
    "Favor 'short' for the shortest interval available e.g. 6h, 'long' for longest interval e.g. 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favor = 'short' #'long'\n",
    "\n",
    "flist = []\n",
    "for site in site_list:\n",
    "    \n",
    "    site_files = glob(obdir + 'combined/%s*.pd'%site)\n",
    "    \n",
    "    if len(site_files) > 1:\n",
    "        if favor == 'short':\n",
    "            flist.append(\n",
    "                site_files[np.argmin([int(f.split('.')[-2].replace('h', '')) \n",
    "                    for f in site_files])])\n",
    "            \n",
    "        elif favor == 'long':\n",
    "            np.argmax([int(f.split('.')[-2].replace('h', '')) for f in site_files])\n",
    "    \n",
    "    else:\n",
    "        flist.append(site_files[0])\n",
    "        \n",
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish which model variables and levels to use\n",
    "This is entirely based on the model that the SVR will be applied to - maximize the predictor set<br>\n",
    "Helpful to open a sample GFS/NAM/HRRR grid or profile to derive the variable list from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we can reuse the fixed list from the proof-of-concept script\n",
    "# But this needs to be made flexible\n",
    "gfs_keys = [\n",
    "       'VO1000', 'VO400', 'VO450', 'VO500', 'VO550', 'VO600', 'VO650',\n",
    "       'VO700', 'VO750', 'VO800', 'VO850', 'VO900', 'VO925', 'VO950',\n",
    "       'VO975', 'CAPE', 'DIR1000', 'DIR400', 'DIR450', 'DIR500', 'DIR550',\n",
    "       'DIR600', 'DIR650', 'DIR700', 'DIR750', 'DIR800', 'DIR850',\n",
    "       'DIR900', 'DIR925', 'DIR950', 'DIR975', 'Z1000', 'Z400', 'Z450',\n",
    "       'Z500', 'Z550', 'Z600', 'Z650', 'Z700', 'Z750', 'Z800', 'Z850',\n",
    "       'Z900', 'Z925', 'Z950', 'Z975', 'RH1000', 'RH400', 'RH450',\n",
    "       'RH500', 'RH550', 'RH600', 'RH650', 'RH700', 'RH750', 'RH800',\n",
    "       'RH850', 'RH900', 'RH925', 'RH950', 'RH975', 'SP', 'SPD1000',\n",
    "       'SPD400', 'SPD450', 'SPD500', 'SPD550', 'SPD600', 'SPD650',\n",
    "       'SPD700', 'SPD750', 'SPD800', 'SPD850', 'SPD900', 'SPD925',\n",
    "       'SPD950', 'SPD975', 'T1000', '2T', 'T400', 'T450', 'T500', 'T550',\n",
    "       'T600', 'T650', 'T700', 'T750', 'T800', 'T850', 'T900', 'T925',\n",
    "       'T950', 'T975', 'TSFC', 'U1000', 'U400', 'U450', 'U500', 'U550',\n",
    "       'U600', 'U650', 'U700', 'U750', 'U800', 'U850', 'U900', 'U925',\n",
    "       'U950', 'U975', 'V1000', 'V400', 'V450', 'V500', 'V550', 'V600',\n",
    "       'V650', 'V700', 'V750', 'V800', 'V850', 'V900', 'V925', 'V950',\n",
    "       'V975', 'W1000', 'W400', 'W450', 'W500', 'W550', 'W600', 'W650',\n",
    "       'W700', 'W750', 'W800', 'W850', 'W900', 'W925', 'W950', 'W975']\n",
    "\n",
    "gfs_levels = [400, 450, 500,550,600, 650, 700, 750, \n",
    "              800, 850, 900, 925, 950, 975, 1000]\n",
    "\n",
    "exclude_keys = []#['Z', 'U', 'V', '2T', 'TSFC', 'SP', 'CAPE', 'DIR', 'SPD', 'RH', 'W']\n",
    "\n",
    "exclude_keys = np.array([[k if ex in k[:len(ex)] else np.nan \n",
    "                          for ex in exclude_keys] for k in gfs_keys])\n",
    "exclude_keys = exclude_keys.flatten()\n",
    "exclude_keys = exclude_keys[exclude_keys != 'nan']\n",
    "\n",
    "gfs_keys = np.array([k for k in gfs_keys if k not in exclude_keys])\n",
    "gfs_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the compiled data\n",
    "Prune unused variables on import based on the list above<br>\n",
    "Sub-surface levels have already been dealt with in the compilation script - modify methods there<br>\n",
    "Note that 12 vs 24 hour intervals have also been dealt with above, if both desired... write that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_var_type = ['mean']#, 'max', 'min']\n",
    "\n",
    "data = []\n",
    "for f in flist:\n",
    "    \n",
    "    site = f.split('/')[-1].split('_')[0]\n",
    "    interval = int(f.split('/')[-1].split('.')[-2].replace('h', ''))\n",
    "    \n",
    "    df = pd.read_pickle(f)\n",
    "\n",
    "    keys = ['slr%d'%interval, 'swe%d_mm'%interval]\n",
    "    \n",
    "    keys.extend(np.hstack([[k for k in df.keys() \n",
    "                  if ((vt in k) & (k.split('_')[0] in gfs_keys))] \n",
    "                 for vt in use_var_type]))\n",
    "\n",
    "    df = df.loc[:, keys].rename(columns={[k for k in keys if 'swe' in k][0]:'swe_mm'})\n",
    "    df = df.rename(columns={[k for k in keys if 'slr' in k][0]:'slr'})\n",
    "    \n",
    "    # df.insert(0, 'site', np.full(df.index.size, fill_value=site, dtype='U10'))\n",
    "    doy = [int(pd.to_datetime(d).strftime('%j')) for d in df.index]\n",
    "    df.insert(2, 'day_of_year', doy)\n",
    "    \n",
    "    data.append(df.reset_index().drop(columns='time'))\n",
    "\n",
    "data = pd.concat(data, sort=False)\n",
    "\n",
    "# Treat the mean value as the instantaneous value for later applications,\n",
    "# we can change this behavior later on if desired. \n",
    "# An alternate method would be to keep the 'mean' tag through training \n",
    "# and choose behavior upon application\n",
    "data = data.rename(columns={k:k.replace('_mean', '') for k in data.keys()})\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard trim data based on criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_slr = 30\n",
    "max_T650 = 0 + 273.15\n",
    "min_swe_mm = 2.54\n",
    "\n",
    "data = data[data['slr'] <= max_slr]\n",
    "data = data[data['T650'] <= max_T650]\n",
    "data = data[data['swe_mm'] >= min_swe_mm]\n",
    "\n",
    "data = data[[k for k in data.keys() if 'swe' not in k]]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate basic bulk statistics and linear correlations\n",
    "Quanitfy how certain variables are related and produce plots to visualize this<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removed: Normalize/Standardize\n",
    "Moved down to where datasets are split. If need be we can normalize for the plots below here too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix, R2 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Correlation Verical Plots, R2 Vertical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify autocorrelated and other dependent predictors\n",
    "Mask these if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test/Validate Sets\n",
    "If we want to expand the hyperparameter tuning, we can test for sensitivity to random sample<br>\n",
    "by looping from bottom of code block back to here<br>\n",
    "*INVESTIGATE: Behavior of random_state in TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets\n",
    "train_size, test_size = 0.80, 0.20\n",
    "X_train, X_test = train_test_split(data.dropna(), test_size=test_size, train_size=train_size, random_state=None)\n",
    "\n",
    "# Perform a secondary split if separate validation set required\n",
    "\n",
    "# Split off the target variable now that TTsplit is done\n",
    "y_train, y_test = X_train.pop('slr'), X_test.pop('slr')\n",
    "\n",
    "print('Train: {}\\nTest: {}\\nValidate: {}'.format(X_train.shape[0], X_test.shape[0], None))\n",
    "\n",
    "train_stats = X_train.describe().T\n",
    "train_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize/Standardize the data\n",
    "We are using z-score normalization for now but other methods exist<br>\n",
    "This is an absolute must for working with any of the ML models available to us<br>\n",
    "Normalize based on the TRAINING dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x, mu, sigma):\n",
    "    '''Standardization'''\n",
    "    return (x - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = zscore(X_train, train_stats['mean'], train_stats['std'])\n",
    "X_test_norm = zscore(X_test, train_stats['mean'], train_stats['std'])\n",
    "\n",
    "X_train_norm.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out plots describing the variability in the data (+ before/after normalization?)<br>\n",
    "It would be valuable to do a similar set of/novel comparison plots with the ERA5 vs GFS<br>\n",
    "for each variable/level in the validation script (separate from and following this script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF/PCA Reduction of highly dimensional data\n",
    "There are implementations of atmospheric data in ML models that heavily leverage<br>\n",
    "EOFs to reduce highly dimensional data to simpler components<br>\n",
    "Evaluate if this is viable here as it will be applied to raw variables later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LASSO/Ridge regression to determine predictor rank/impact\n",
    "L1/L2 Parameter Tuning -- Use to select relevant predictors, reduce irrelevant to zero, rank by influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_norm, y_train)\n",
    "\n",
    "lr_train_score = lr.score(X_train_norm, y_train)\n",
    "lr_test_score = lr.score(X_test_norm, y_test)\n",
    "\n",
    "print('Linear Regression Train Score: %.3f'%lr_train_score)\n",
    "print('Linear Regression Test Score: %.3f'%lr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "ranksort = np.argsort(abs(lr.coef_))[::-1]\n",
    "lr_coefs = lr.coef_[ranksort]\n",
    "lr_keys = X_train_norm.keys()[ranksort]\n",
    "mask = lr_coefs != 0\n",
    "\n",
    "ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "ax.barh(lr_keys[mask], lr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "for i, k in enumerate(lr_keys[mask]):\n",
    "    if lr_coefs[i] != 0:\n",
    "        ax.text(lr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title('Linear Regression Coefs')\n",
    "ax.set_xlabel('Linear Coef Value')\n",
    "ax.grid(zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# The higher the alpha value, more restriction on the coefficients; \n",
    "# low alpha > more generalization, coefficients are barely restricted\n",
    "rr_alpha_tune = []\n",
    "for alpha in np.append(np.arange(0.01, 1, 0.01), np.arange(1, 1000, 1)):\n",
    "\n",
    "    rr = Ridge(alpha=alpha)\n",
    "    rr.fit(X_train, y_train)\n",
    "\n",
    "    rr_train_score = rr.score(X_train, y_train)\n",
    "    rr_test_score = rr.score(X_test, y_test)\n",
    "    \n",
    "    rr_alpha_tune.append([alpha, rr_train_score, rr_test_score, rr])\n",
    "rr_alpha_tune = np.array(rr_alpha_tune)\n",
    "\n",
    "alpha, rr_train_score, rr_test_score, rr = rr_alpha_tune[np.argmax(rr_alpha_tune[:, 2])]\n",
    "print('Ridge Regression alpha: %.3f'%alpha)\n",
    "print('Ridge Regression Train Score: %.3f'%rr_train_score)\n",
    "print('Ridge Regression Test Score: %.3f'%rr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "ranksort = np.argsort(abs(rr.coef_))[::-1]\n",
    "rr_coefs = rr.coef_[ranksort]\n",
    "rr_keys = X_train_norm.keys()[ranksort]\n",
    "mask = rr_coefs != 0\n",
    "\n",
    "ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "ax.barh(rr_keys[mask], rr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "for i, k in enumerate(rr_keys[mask]):\n",
    "    if rr_coefs[i] != 0:\n",
    "        ax.text(rr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title('Ridge Regression Coefs')\n",
    "ax.set_xlabel('Ridge Coef Value')\n",
    "ax.grid(zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Lasso\n",
    "\n",
    "# lasso_alpha_tune = []\n",
    "# for alpha in np.arange(0.001, .01, 0.001):\n",
    "\n",
    "#     lasso = Lasso(alpha=alpha, max_iter=10e5)\n",
    "#     lasso.fit(X_train_norm, y_train)\n",
    "\n",
    "#     lasso_train_score = lasso.score(X_train_norm, y_train)\n",
    "#     lasso_test_score = lasso.score(X_test_norm, y_test)\n",
    "#     lasso_coeff_used = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "#     lasso_alpha_tune.append([alpha, lasso_train_score, lasso_test_score, lasso])\n",
    "# lasso_alpha_tune = np.array(lasso_alpha_tune)\n",
    "\n",
    "# alpha, lasso_train_score, lasso_test_score, lasso = lasso_alpha_tune[np.argmax(lasso_alpha_tune[:, 2])]\n",
    "# print('Lasso alpha: %.3f'%alpha)\n",
    "# print('Lasso Train Score: %.3f'%lasso_train_score)\n",
    "# print('Lasso Test Score: %.3f'%lasso_test_score)\n",
    "# print('Number of Features Used: %d'%lasso_coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(lasso.coef_))[::-1]\n",
    "# lasso_coefs = lasso.coef_[ranksort]\n",
    "# lasso_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = lasso_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(lasso_keys[mask], lasso_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(lasso_keys[mask]):\n",
    "#     if lasso_coefs[i] != 0:\n",
    "#         ax.text(lasso_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Lasso Regression Coefs')\n",
    "# ax.set_xlabel('Lasso Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Grid-search for best parameters? (alpha and l1_ratio)\n",
    "\n",
    "elastic = ElasticNet(alpha=0.009, l1_ratio=0.1, max_iter=10e5)\n",
    "elastic.fit(X_train_norm, y_train)\n",
    "\n",
    "elastic_train_score = elastic.score(X_train_norm, y_train)\n",
    "elastic_test_score = elastic.score(X_test_norm, y_test)\n",
    "elastic_coeff_used = np.sum(elastic.coef_ != 0)\n",
    "    \n",
    "print('Elastic Train Score: %.3f'%elastic_train_score)\n",
    "print('Elastic Test Score: %.3f'%elastic_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "ranksort = np.argsort(abs(elastic.coef_))[::-1]\n",
    "elastic_coefs = elastic.coef_[ranksort]\n",
    "elastic_keys = X_train_norm.keys()[ranksort]\n",
    "mask = elastic_coefs != 0\n",
    "\n",
    "ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "ax.barh(elastic_keys[mask], elastic_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "for i, k in enumerate(elastic_keys[mask]):\n",
    "    if elastic_coefs[i] != 0:\n",
    "        ax.text(elastic_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "ax.set_title('Elastic Regression Coefs')\n",
    "ax.set_xlabel('Elastic Coef Value')\n",
    "ax.grid(zorder=-1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "for layer_sizes in [2]:\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(layer_sizes,), \n",
    "        activation='relu', \n",
    "        solver='sgd', \n",
    "        alpha=0.00001, \n",
    "        batch_size='auto', \n",
    "        learning_rate='adaptive', \n",
    "        learning_rate_init=0.0001, \n",
    "        power_t=0.5, \n",
    "        max_iter=50000, \n",
    "        shuffle=True, \n",
    "        random_state=None, \n",
    "        tol=0.00001, \n",
    "        verbose=False, \n",
    "        warm_start=False, \n",
    "        momentum=0.85, \n",
    "        nesterovs_momentum=True, \n",
    "        early_stopping=False, \n",
    "        validation_fraction=0.35, \n",
    "        beta_1=0.5, \n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-08)\n",
    "\n",
    "    mlp.fit(X_train_norm, y_train)\n",
    "\n",
    "    mlp_train_score = mlp.score(X_train_norm, y_train)\n",
    "    mlp_test_score = mlp.score(X_test_norm, y_test)\n",
    "    # mlp_coeff_used = np.sum(mlp.coef_ != 0)\n",
    "\n",
    "    print('Layer Sizes: %d'%layer_sizes)\n",
    "    print('MLP Train Score: %.3f'%mlp_train_score)\n",
    "    print('MLP Test Score: %.3f'%mlp_test_score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to use feature selection, we can do so here.\n",
    "Can choose the model from which to take features, do so manually, or not at all (set equal to X_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = elastic_keys[mask]\n",
    "# feature_selection\n",
    "\n",
    "# pre_select = X_train_norm.keys()\n",
    "# X_train_norm = X_train_norm[feature_selection]\n",
    "# X_test_norm = X_test_norm[feature_selection]\n",
    "# post_select = X_train_norm.keys()\n",
    "\n",
    "# print([k for k in pre_select if k not in post_select])\n",
    "\n",
    "# print(X_train_norm.shape)\n",
    "# print(X_test_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Support Vector Regression Model\n",
    "(Other ML models like ANN from keras, tensorflow, pytorch may be used in this block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a K-Fold Cross Validation to assess model performance (Optional, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the SVR model multiprocess friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVR_mp(_params):\n",
    "    \n",
    "    print('.', end='')\n",
    "    \n",
    "    _i, _j, _C, _e = _params\n",
    "    \n",
    "    _model = SVR(\n",
    "                C=_C, #Ridge regularization parameter for (L2)^2 penalty\n",
    "                epsilon=_e, #Specifies the epsilon-tube within which no penalty is associated in the training loss function\n",
    "                kernel='rbf', #'linear', 'polynomial', 'rbf'\n",
    "                degree=3, #pass interger for 'polynomial' kernel, ignored otherwise\n",
    "                tol=0.001, #stopping tolerance\n",
    "                shrinking=False, \n",
    "                cache_size=200, \n",
    "                verbose=False)\n",
    "    \n",
    "    _model.fit(X_train_norm, y_train)\n",
    "    \n",
    "    test_predictions = _model.predict(X_test_norm).flatten()\n",
    "    _r2 = _model.score(X_test_norm, y_test) #sklearn.metrics.r2_score(y_test.values.flatten(), test_predictions)\n",
    "    _mse = sklearn.metrics.mean_squared_error(y_test.values.flatten(), test_predictions)\n",
    "    _mae = sklearn.metrics.mean_absolute_error(y_test.values.flatten(), test_predictions)\n",
    "    \n",
    "    return (_i, _j, _C, _e, _r2, _mae, _mse, _model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from sklearn.svm import SVR\n",
    "import sklearn\n",
    "\n",
    "# Best test\n",
    "# crange = np.arange(1, 60, 1)\n",
    "# erange = np.arange(0.0, 5.1, .1)\n",
    "\n",
    "crange = np.arange(1, 25, 1)\n",
    "erange = np.arange(0.1, 3.1, .1)\n",
    "\n",
    "params = {}\n",
    "params['r2'] = np.zeros((len(crange), len(erange)))\n",
    "params['mae'] = np.zeros((len(crange), len(erange)))\n",
    "params['mse'] = np.zeros((len(crange), len(erange)))\n",
    "params['model'] = np.empty((len(crange), len(erange)), dtype='object')\n",
    "params['epsilon'] = np.zeros((len(crange), len(erange)))\n",
    "params['C'] = np.zeros((len(crange), len(erange)))\n",
    "\n",
    "mp_params = np.array([[(i, j, C, e) for j, e in enumerate(erange)] \n",
    "                      for i, C in enumerate(crange)]).reshape(-1, 4)\n",
    "\n",
    "print('Iterations to attempt: %d'%len(mp_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVR, collect output iterated over hyperparameters (C, epsilon)<br>\n",
    "Specify the cost function/loss function used, tolerances, kernel, error metric, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paralellize here\n",
    "# mp_returns = [SVR_mp(_param) for _param in mp_params[:10]]\n",
    "\n",
    "with mp.get_context('fork').Pool(64) as p:\n",
    "    mp_returns = p.map(SVR_mp, mp_params, chunksize=1)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "for item in mp_returns:\n",
    "    \n",
    "    i, j, C, e, r2, mae, mse, model = item\n",
    "    i, j = int(i), int(j)\n",
    "        \n",
    "    params['r2'][i, j] = r2\n",
    "    params['mse'][i, j] = mse\n",
    "    params['mae'][i, j] = mae\n",
    "    params['model'][i, j] = model\n",
    "    params['epsilon'][i, j] = e\n",
    "    params['C'][i, j] = C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model performance over time, cost/loss function evolution and skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_on, indexer, _ = 'R2', np.where(params['r2'] == params['r2'].max()), params['r2'].max()\n",
    "min_on, indexer, _ = 'MAE', np.where(params['mae'] == params['mae'].min()), params['mae'].min()\n",
    "min_on, indexer, _ = 'MSE', np.where(params['mse'] == params['mse'].min()), params['mse'].min()\n",
    "\n",
    "def MARE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "for min_on in ['mse', 'mae', 'r2']:\n",
    "    \n",
    "    if min_on in ['mse', 'mae']:\n",
    "        min_max = 'Minimized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].min())\n",
    "    elif min_on in ['r2']:\n",
    "        min_max = 'Maximized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].max())\n",
    "\n",
    "    r, c = indexer\n",
    "    r, c = r[0], c[0]\n",
    "    r, c, _\n",
    "\n",
    "    model = params['model'][r, c]\n",
    "    test_predictions = model.predict(X_test_norm)\n",
    "    \n",
    "    y_true = y_test\n",
    "    y_pred = test_predictions\n",
    "    print('MARE ', MARE(y_true, y_pred))\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, facecolor='w', figsize=(24, 12))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    ax = axs[0]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mae'])\n",
    "    plt.colorbar(cbar, label='mae', ax=ax)\n",
    "    ax.set_title('Min MAE: %.3f'%params['mae'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[1]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mse'])\n",
    "    plt.colorbar(cbar, label='mse', ax=ax)\n",
    "    ax.set_title('Min MSE: %.3f'%params['mse'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[2]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['r2'])\n",
    "    plt.colorbar(cbar, label='r2', ax=ax)\n",
    "    ax.set_title('Max R^2: %.3f'%params['r2'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='k', marker='+')\n",
    "\n",
    "    for ax in axs[:3]:\n",
    "        ax.set_xlabel('epsilon')\n",
    "        ax.set_ylabel('C_val')\n",
    "        ax.set_ylim([crange.min(), crange.max()])\n",
    "        ax.set_xlim([erange.min(), erange.max()])\n",
    "\n",
    "    ax = axs[3]\n",
    "    maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "    ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "    ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "    ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[4]\n",
    "    maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "    maxslr += 5\n",
    "    ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "    ax.set_xlabel('Observed SLR')\n",
    "    ax.set_ylabel('Predicted SLR')\n",
    "    ax.plot([0, maxslr], [0, maxslr])\n",
    "    ax.set_xlim([0, maxslr])\n",
    "    ax.set_ylim([0, maxslr])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[5]\n",
    "    error = test_predictions - y_test\n",
    "    ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "    ax.set_xlabel('Prediction Error')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid()\n",
    "\n",
    "    plt.suptitle('Support Vector Regression Model\\n%s on: %s\\n\\nepsilon %.3f\\nc_val: %.3f'%(min_max, min_on.upper(), params['epsilon'][r, c], params['C'][r, c]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot SVR output with hyperparameters (C, epsilon)<br>\n",
    "Apply a grid-search method to select best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranksort = np.argsort(abs(model.coef_))[::-1]\n",
    "# svr_coefs = model.coef_[0][ranksort][0]\n",
    "# svr_keys = X_train_norm.keys()[ranksort][0]\n",
    "# mask = svr_coefs != 0\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(svr_keys[mask], svr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(svr_keys):\n",
    "#     if svr_coefs[i] != 0:\n",
    "#         ax.text(svr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# # ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('SVR Coefs')\n",
    "# ax.set_xlabel('SVR Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe final trained SVR model\n",
    "Descriptive plots of predictor coefficient rank/influence<br>\n",
    "Print out model parameters, error metrics, skill scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out a usable model along with the input parameters for application\n",
    "Save with a descriptive filename and a metadata text file!<br>\n",
    "This will make swapping out models for evaluation much simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
