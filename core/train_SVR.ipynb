{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which sites to train the SVR model on\n",
    "Can be all available, or a defined list 'site_list' passed, or single station as list e.g. ['CLN']<br>\n",
    "Use all with an exclude list is another alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AGD', 'ALTA', 'BCC', 'BSNFDC', 'BSNFEX', 'BSNFJE', 'CLNX', 'PVC',\n",
       "       'SLB'], dtype='<U6')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flist = glob(obdir + 'combined/*.pd')\n",
    "\n",
    "# This can be a manual site list if desired\n",
    "site_list = np.unique([f.split('/')[-1].split('_')[0] for f in flist])\n",
    "\n",
    "# site_list = [s for s in site_list if 'BSNF' not in s]\n",
    "#site_list = ['CLN', 'AGD', 'ALTA']\n",
    "\n",
    "site_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each site, choose which files to use\n",
    "For now, we will only use one set of observations from each site so samples remain independent<br>\n",
    "We can change this behavior down the road to use all or some intervals<br>\n",
    "Favor 'short' for the shortest interval available e.g. 6h, 'long' for longest interval e.g. 24h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/AGD_2004_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/AGD_2004_2019.12h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/ALTA_1980_2020.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/BCC_2004_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/BCC_2004_2019.12h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/BSNFDC_2005_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/BSNFEX_2012_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/BSNFJE_2005_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/CLNX_1999_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/CLNX_1999_2019.12h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/PVC_2006_2019.24h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/PVC_2006_2019.12h.pd',\n",
       "       '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/combined/SLB_1980_2020.24h.pd'],\n",
       "      dtype='<U106')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favor = 'short' #'long'\n",
    "\n",
    "flist = []\n",
    "for site in site_list:\n",
    "    \n",
    "    site_files = glob(obdir + 'combined/%s*.pd'%site)\n",
    "    \n",
    "#     if len(site_files) > 1:\n",
    "#         if favor == 'short':\n",
    "#             flist.append(\n",
    "#                 site_files[np.argmin([int(f.split('.')[-2].replace('h', '')) \n",
    "#                     for f in site_files])])\n",
    "            \n",
    "#         elif favor == 'long':\n",
    "#             np.argmax([int(f.split('.')[-2].replace('h', '')) for f in site_files])\n",
    "    \n",
    "#     else:\n",
    "    flist.append(site_files)#[0])\n",
    "        \n",
    "flist = np.hstack(flist)\n",
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish which model variables and levels to use\n",
    "This is entirely based on the model that the SVR will be applied to - maximize the predictor set<br>\n",
    "Helpful to open a sample GFS/NAM/HRRR grid or profile to derive the variable list from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the compiled data\n",
    "Prune unused variables on import based on the list above<br>\n",
    "Sub-surface levels have already been dealt with in the compilation script - modify methods there<br>\n",
    "Note that 12 vs 24 hour intervals have also been dealt with above, if both desired... write that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14340, 52)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slr</th>\n",
       "      <th>swe_mm</th>\n",
       "      <th>T_01agl</th>\n",
       "      <th>U_01agl</th>\n",
       "      <th>V_01agl</th>\n",
       "      <th>Z_01agl</th>\n",
       "      <th>R_01agl</th>\n",
       "      <th>T_02agl</th>\n",
       "      <th>U_02agl</th>\n",
       "      <th>V_02agl</th>\n",
       "      <th>...</th>\n",
       "      <th>T_09agl</th>\n",
       "      <th>U_09agl</th>\n",
       "      <th>V_09agl</th>\n",
       "      <th>Z_09agl</th>\n",
       "      <th>R_09agl</th>\n",
       "      <th>T_10agl</th>\n",
       "      <th>U_10agl</th>\n",
       "      <th>V_10agl</th>\n",
       "      <th>Z_10agl</th>\n",
       "      <th>R_10agl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.88</td>\n",
       "      <td>12.95</td>\n",
       "      <td>274.660889</td>\n",
       "      <td>-7.088934</td>\n",
       "      <td>10.160769</td>\n",
       "      <td>30329.960938</td>\n",
       "      <td>75.513489</td>\n",
       "      <td>271.041412</td>\n",
       "      <td>-3.992080</td>\n",
       "      <td>12.588694</td>\n",
       "      <td>...</td>\n",
       "      <td>232.852051</td>\n",
       "      <td>-6.405712</td>\n",
       "      <td>26.100204</td>\n",
       "      <td>92202.718750</td>\n",
       "      <td>60.803410</td>\n",
       "      <td>224.151947</td>\n",
       "      <td>-4.833603</td>\n",
       "      <td>25.590794</td>\n",
       "      <td>104153.101562</td>\n",
       "      <td>49.459042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.32</td>\n",
       "      <td>11.94</td>\n",
       "      <td>264.753296</td>\n",
       "      <td>4.310259</td>\n",
       "      <td>-6.581493</td>\n",
       "      <td>29981.410156</td>\n",
       "      <td>71.208900</td>\n",
       "      <td>263.660675</td>\n",
       "      <td>5.649075</td>\n",
       "      <td>-11.672703</td>\n",
       "      <td>...</td>\n",
       "      <td>226.124588</td>\n",
       "      <td>4.332445</td>\n",
       "      <td>-28.059399</td>\n",
       "      <td>90629.929688</td>\n",
       "      <td>80.730492</td>\n",
       "      <td>216.126663</td>\n",
       "      <td>3.423474</td>\n",
       "      <td>-34.172352</td>\n",
       "      <td>102199.382812</td>\n",
       "      <td>93.531425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.21</td>\n",
       "      <td>21.84</td>\n",
       "      <td>269.180389</td>\n",
       "      <td>8.406146</td>\n",
       "      <td>-2.683464</td>\n",
       "      <td>30030.525391</td>\n",
       "      <td>91.871696</td>\n",
       "      <td>266.840668</td>\n",
       "      <td>11.762459</td>\n",
       "      <td>-7.780785</td>\n",
       "      <td>...</td>\n",
       "      <td>229.437149</td>\n",
       "      <td>17.053003</td>\n",
       "      <td>-9.621124</td>\n",
       "      <td>91323.421875</td>\n",
       "      <td>79.696442</td>\n",
       "      <td>219.605240</td>\n",
       "      <td>19.694160</td>\n",
       "      <td>-9.853032</td>\n",
       "      <td>103065.382812</td>\n",
       "      <td>79.703049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.20</td>\n",
       "      <td>25.40</td>\n",
       "      <td>269.066162</td>\n",
       "      <td>8.432843</td>\n",
       "      <td>-0.374978</td>\n",
       "      <td>29730.644531</td>\n",
       "      <td>91.111023</td>\n",
       "      <td>265.930908</td>\n",
       "      <td>11.585249</td>\n",
       "      <td>-3.159128</td>\n",
       "      <td>...</td>\n",
       "      <td>227.716583</td>\n",
       "      <td>22.342390</td>\n",
       "      <td>0.114759</td>\n",
       "      <td>90622.953125</td>\n",
       "      <td>84.874001</td>\n",
       "      <td>220.347107</td>\n",
       "      <td>24.896339</td>\n",
       "      <td>-1.074835</td>\n",
       "      <td>102320.210938</td>\n",
       "      <td>61.295341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.34</td>\n",
       "      <td>13.21</td>\n",
       "      <td>263.492554</td>\n",
       "      <td>7.508132</td>\n",
       "      <td>-4.722282</td>\n",
       "      <td>29630.097656</td>\n",
       "      <td>68.908722</td>\n",
       "      <td>260.387848</td>\n",
       "      <td>9.471686</td>\n",
       "      <td>-6.016488</td>\n",
       "      <td>...</td>\n",
       "      <td>226.809479</td>\n",
       "      <td>36.793125</td>\n",
       "      <td>-22.943510</td>\n",
       "      <td>89165.539062</td>\n",
       "      <td>47.654980</td>\n",
       "      <td>220.883804</td>\n",
       "      <td>37.234291</td>\n",
       "      <td>-25.007586</td>\n",
       "      <td>100866.343750</td>\n",
       "      <td>41.149246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     slr  swe_mm     T_01agl   U_01agl    V_01agl       Z_01agl    R_01agl  \\\n",
       "0   5.88   12.95  274.660889 -7.088934  10.160769  30329.960938  75.513489   \n",
       "1   5.32   11.94  264.753296  4.310259  -6.581493  29981.410156  71.208900   \n",
       "2  12.21   21.84  269.180389  8.406146  -2.683464  30030.525391  91.871696   \n",
       "3  12.20   25.40  269.066162  8.432843  -0.374978  29730.644531  91.111023   \n",
       "4  16.34   13.21  263.492554  7.508132  -4.722282  29630.097656  68.908722   \n",
       "\n",
       "      T_02agl    U_02agl    V_02agl  ...     T_09agl    U_09agl    V_09agl  \\\n",
       "0  271.041412  -3.992080  12.588694  ...  232.852051  -6.405712  26.100204   \n",
       "1  263.660675   5.649075 -11.672703  ...  226.124588   4.332445 -28.059399   \n",
       "2  266.840668  11.762459  -7.780785  ...  229.437149  17.053003  -9.621124   \n",
       "3  265.930908  11.585249  -3.159128  ...  227.716583  22.342390   0.114759   \n",
       "4  260.387848   9.471686  -6.016488  ...  226.809479  36.793125 -22.943510   \n",
       "\n",
       "        Z_09agl    R_09agl     T_10agl    U_10agl    V_10agl        Z_10agl  \\\n",
       "0  92202.718750  60.803410  224.151947  -4.833603  25.590794  104153.101562   \n",
       "1  90629.929688  80.730492  216.126663   3.423474 -34.172352  102199.382812   \n",
       "2  91323.421875  79.696442  219.605240  19.694160  -9.853032  103065.382812   \n",
       "3  90622.953125  84.874001  220.347107  24.896339  -1.074835  102320.210938   \n",
       "4  89165.539062  47.654980  220.883804  37.234291 -25.007586  100866.343750   \n",
       "\n",
       "     R_10agl  \n",
       "0  49.459042  \n",
       "1  93.531425  \n",
       "2  79.703049  \n",
       "3  61.295341  \n",
       "4  41.149246  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_var_type = ['mean']#, 'max', 'min']\n",
    "\n",
    "data = []\n",
    "for f in flist:\n",
    "    \n",
    "    site = f.split('/')[-1].split('_')[0]\n",
    "    interval = int(f.split('/')[-1].split('.')[-2].replace('h', ''))\n",
    "    \n",
    "    df = pd.read_pickle(f)\n",
    "    \n",
    "    include_keys = ['T', 'R', 'U', 'V', 'Z']\n",
    "    keys = ['slr', 'swe_mm']\n",
    "    keys.extend(np.hstack([[k for k in df.keys()\n",
    "                            if ((vt in k) & (k.split('_')[0] in include_keys))] \n",
    "                           for vt in use_var_type]))\n",
    "        \n",
    "    df = df.loc[:, keys].rename(columns={[k for k in keys if 'swe' in k][0]:'swe_mm'})\n",
    "    df = df.loc[:, :].rename(columns={[k for k in keys if 'swe' in k][0]:'swe_mm'})\n",
    "    df = df.rename(columns={[k for k in keys if 'slr' in k][0]:'slr'})\n",
    "    df = df.drop(columns=[k for k in keys if 'auto' in k])\n",
    "    \n",
    "    # df.insert(0, 'site', np.full(df.index.size, fill_value=site, dtype='U10'))\n",
    "    doy = [int(pd.to_datetime(d).strftime('%j')) for d in df.index]\n",
    "    #df.insert(2, 'day_of_year', doy)\n",
    "    \n",
    "    data.append(df.reset_index().drop(columns='time'))\n",
    "\n",
    "data = pd.concat(data, sort=False)\n",
    "\n",
    "# Treat the mean value as the instantaneous value for later applications,\n",
    "# we can change this behavior later on if desired. \n",
    "# An alternate method would be to keep the 'mean' tag through training \n",
    "# and choose behavior upon application\n",
    "data = data.rename(columns={k:k.replace('_mean', '') for k in data.keys()})\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard trim data based on criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c54249c5376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmin_swe_mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.54\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_slr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_slr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T_01agl'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_T650\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "min_slr, max_slr = 2.5, 30\n",
    "max_T650 = 0 + 273.15\n",
    "min_swe_mm = 2.54\n",
    "\n",
    "data = data[data['slr'] >= min_slr]\n",
    "data = data[data['slr'] <= max_slr]\n",
    "data = data[data['T_01agl'] <= max_T650]\n",
    "data = data[data['swe_mm'] >= min_swe_mm]\n",
    "\n",
    "dkeys = [k for k in data.keys() if 'T_' in k]\n",
    "dkeys.insert(0, 'slr')\n",
    "\n",
    "data = data[dkeys]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, facecolor='w', figsize=(10, 8))\n",
    "\n",
    "data['slr'].hist(ax=ax, color='k', width=1.8, bins=np.arange(0, 51, 2))\n",
    "\n",
    "ax.set_xticks(np.arange(0, 51, 2))\n",
    "ax.set_xlim([0, 50])\n",
    "ax.grid(False)\n",
    "ax.set_title('n=%d\\n%s - %s'%(len(data['slr']), 1980, 2020))\n",
    "\n",
    "stats = [np.nanmean(data['slr']), np.nanstd(data['slr']), np.percentile(data['slr'], [75, 50, 25])]\n",
    "stats\n",
    "\n",
    "ax.text(x=30, y=1000, s='Average SLR: %.1f\\nStandard Dev: %.1f\\n75th Percentile: %.1f\\n50th Percentile: %.1f\\n25th Percentile: %.1f'%(stats[0], stats[1], stats[2][0], stats[2][1], stats[2][2]), fontsize=18, weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate basic bulk statistics and linear correlations\n",
    "Quanitfy how certain variables are related and produce plots to visualize this<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Removed: Normalize/Standardize\n",
    "Moved down to where datasets are split. If need be we can normalize for the plots below here too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix, R2 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Correlation Verical Plots, R2 Vertical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify autocorrelated and other dependent predictors\n",
    "Mask these if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test/Validate Sets\n",
    "If we want to expand the hyperparameter tuning, we can test for sensitivity to random sample<br>\n",
    "by looping from bottom of code block back to here<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(slr) for stratification (> 1 ct per class label)\n",
    "data = data.dropna()\n",
    "fac = 5\n",
    "slr = np.round(data['slr']/fac, 0)*fac\n",
    "slr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total: %d'%len(data))\n",
    "\n",
    "# Split into train/test sets\n",
    "train_size, test_size, random_state = None, 0.33, 5\n",
    "X_train, X_test = train_test_split(data, \n",
    "                                       test_size=test_size, train_size=train_size, \n",
    "                                       random_state=random_state, stratify=slr)\n",
    "\n",
    "# Perform a secondary split if separate validation set required\n",
    "\n",
    "# Split off the target variable now that TTsplit is done\n",
    "y_train, y_test = X_train.pop('slr'), X_test.pop('slr')\n",
    "# y_train = np.round(y_train/fac, 0)*fac\n",
    "\n",
    "print('Train: {}\\nTest: {}\\nValidate: {}'.format(X_train.shape[0], X_test.shape[0], None))\n",
    "\n",
    "train_stats = X_train.describe().T\n",
    "train_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize/Standardize the data\n",
    "We are using z-score normalization for now but other methods exist<br>\n",
    "This is an absolute must for working with any of the ML models available to us<br>\n",
    "Normalize based on the TRAINING dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(quantile_range=(25, 75)).fit(X_train)\n",
    "\n",
    "X_train_norm = pd.DataFrame(scaler.transform(X_train.loc[:, list(X_train.keys())]), columns=X_train.keys())\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test.loc[:, list(X_train.keys())]), columns=X_train.keys())\n",
    "\n",
    "X_train_norm.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out plots describing the variability in the data (+ before/after normalization?)<br>\n",
    "It would be valuable to do a similar set of/novel comparison plots with the ERA5 vs GFS<br>\n",
    "for each variable/level in the validation script (separate from and following this script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF/PCA Reduction of highly dimensional data\n",
    "There are implementations of atmospheric data in ML models that heavily leverage<br>\n",
    "EOFs to reduce highly dimensional data to simpler components<br>\n",
    "Evaluate if this is viable here as it will be applied to raw variables later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LASSO/Ridge regression to determine predictor rank/impact\n",
    "L1/L2 Parameter Tuning -- Use to select relevant predictors, reduce irrelevant to zero, rank by influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_norm, y_train)\n",
    "\n",
    "lr_train_score = lr.score(X_train_norm, y_train)\n",
    "lr_test_score = lr.score(X_test_norm, y_test)\n",
    "\n",
    "print('Linear Regression Train Score: %.3f'%lr_train_score)\n",
    "print('Linear Regression Test Score: %.3f'%lr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(lr.coef_))[::-1]\n",
    "# lr_coefs = lr.coef_[ranksort]\n",
    "# lr_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = lr_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(lr_keys[mask], lr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(lr_keys[mask]):\n",
    "#     if lr_coefs[i] != 0:\n",
    "#         ax.text(lr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Linear Regression Coefs')\n",
    "# ax.set_xlabel('Linear Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lr.predict(X_test_norm)\n",
    "lr_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "ax = axs[0]\n",
    "maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "ax.grid()\n",
    "\n",
    "ax = axs[1]\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "ax.set_xlabel('Observed SLR')\n",
    "ax.set_ylabel('Predicted SLR')\n",
    "ax.plot([0, maxslr], [0, maxslr])\n",
    "ax.set_xlim([0, maxslr])\n",
    "ax.set_ylim([0, maxslr])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "axs[1].set_title('Linear Model\\nR2: %.3f\\nMAE: %.3f'%(lr_test_score, lr_mae))\n",
    "\n",
    "ax = axs[2]\n",
    "error = test_predictions - y_test\n",
    "ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "ax.set_xlabel('Prediction Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid()\n",
    "\n",
    "# fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# The higher the alpha value, more restriction on the coefficients; \n",
    "# low alpha > more generalization, coefficients are barely restricted\n",
    "rr_alpha_tune = []\n",
    "for alpha in np.append(np.arange(0.01, 1, 0.01), np.arange(1, 1000, 1)):\n",
    "\n",
    "    rr = Ridge(alpha=alpha)\n",
    "    rr.fit(X_train, y_train)\n",
    "\n",
    "    rr_train_score = rr.score(X_train, y_train)\n",
    "    rr_test_score = rr.score(X_test, y_test)\n",
    "    \n",
    "    rr_alpha_tune.append([alpha, rr_train_score, rr_test_score, rr])\n",
    "rr_alpha_tune = np.array(rr_alpha_tune)\n",
    "\n",
    "alpha, rr_train_score, rr_test_score, rr = rr_alpha_tune[np.argmax(rr_alpha_tune[:, 2])]\n",
    "print('Ridge Regression alpha: %.3f'%alpha)\n",
    "print('Ridge Regression Train Score: %.3f'%rr_train_score)\n",
    "print('Ridge Regression Test Score: %.3f'%rr_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(rr.coef_))[::-1]\n",
    "# rr_coefs = rr.coef_[ranksort]\n",
    "# rr_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = rr_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(rr_keys[mask], rr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(rr_keys[mask]):\n",
    "#     if rr_coefs[i] != 0:\n",
    "#         ax.text(rr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Ridge Regression Coefs')\n",
    "# ax.set_xlabel('Ridge Coef Value')\n",
    "# ax.grid(zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lr.predict(X_test_norm)\n",
    "rr_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "ax = axs[0]\n",
    "maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "ax.grid()\n",
    "\n",
    "ax = axs[1]\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "ax.set_xlabel('Observed SLR')\n",
    "ax.set_ylabel('Predicted SLR')\n",
    "ax.plot([0, maxslr], [0, maxslr])\n",
    "ax.set_xlim([0, maxslr])\n",
    "ax.set_ylim([0, maxslr])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "axs[1].set_title('Linear Model\\nR2: %.3f\\nMAE: %.3f'%(rr_test_score, rr_mae))\n",
    "\n",
    "ax = axs[2]\n",
    "error = test_predictions - y_test\n",
    "ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "ax.set_xlabel('Prediction Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid()\n",
    "\n",
    "# fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_alpha_tune = []\n",
    "for alpha in np.arange(0.001, .01, 0.001):\n",
    "\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10e5)\n",
    "    lasso.fit(X_train_norm, y_train)\n",
    "\n",
    "    lasso_train_score = lasso.score(X_train_norm, y_train)\n",
    "    lasso_test_score = lasso.score(X_test_norm, y_test)\n",
    "    lasso_coeff_used = np.sum(lasso.coef_ != 0)\n",
    "    \n",
    "    lasso_alpha_tune.append([alpha, lasso_train_score, lasso_test_score, lasso])\n",
    "lasso_alpha_tune = np.array(lasso_alpha_tune)\n",
    "\n",
    "alpha, lasso_train_score, lasso_test_score, lasso = lasso_alpha_tune[np.argmax(lasso_alpha_tune[:, 2])]\n",
    "print('Lasso alpha: %.3f'%alpha)\n",
    "print('Lasso Train Score: %.3f'%lasso_train_score)\n",
    "print('Lasso Test Score: %.3f'%lasso_test_score)\n",
    "print('Number of Features Used: %d'%lasso_coeff_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(lasso.coef_))[::-1]\n",
    "# lasso_coefs = lasso.coef_[ranksort]\n",
    "# lasso_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = lasso_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(lasso_keys[mask], lasso_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(lasso_keys[mask]):\n",
    "#     if lasso_coefs[i] != 0:\n",
    "#         ax.text(lasso_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Lasso Regression Coefs')\n",
    "# ax.set_xlabel('Lasso Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lr.predict(X_test_norm)\n",
    "lasso_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "ax = axs[0]\n",
    "maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "ax.grid()\n",
    "\n",
    "ax = axs[1]\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "ax.set_xlabel('Observed SLR')\n",
    "ax.set_ylabel('Predicted SLR')\n",
    "ax.plot([0, maxslr], [0, maxslr])\n",
    "ax.set_xlim([0, maxslr])\n",
    "ax.set_ylim([0, maxslr])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "axs[1].set_title('Lasso Model\\nR2: %.3f\\nMAE: %.3f'%(lasso_test_score, lasso_mae))\n",
    "\n",
    "ax = axs[2]\n",
    "error = test_predictions - y_test\n",
    "ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "ax.set_xlabel('Prediction Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid()\n",
    "\n",
    "# fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Grid-search for best parameters? (alpha and l1_ratio)\n",
    "\n",
    "elastic = ElasticNet(alpha=0.009, l1_ratio=0.1, max_iter=10e5)\n",
    "elastic.fit(X_train_norm, y_train)\n",
    "\n",
    "elastic_train_score = elastic.score(X_train_norm, y_train)\n",
    "elastic_test_score = elastic.score(X_test_norm, y_test)\n",
    "elastic_coeff_used = np.sum(elastic.coef_ != 0)\n",
    "    \n",
    "print('Elastic Train Score: %.3f'%elastic_train_score)\n",
    "print('Elastic Test Score: %.3f'%elastic_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ranksort = np.argsort(abs(elastic.coef_))[::-1]\n",
    "# elastic_coefs = elastic.coef_[ranksort]\n",
    "# elastic_keys = X_train_norm.keys()[ranksort]\n",
    "# mask = elastic_coefs != 0\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(elastic_keys[mask], elastic_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(elastic_keys[mask]):\n",
    "#     if elastic_coefs[i] != 0:\n",
    "#         ax.text(elastic_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('Elastic Regression Coefs')\n",
    "# ax.set_xlabel('Elastic Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = elastic.predict(X_test_norm)\n",
    "elastic_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "ax = axs[0]\n",
    "maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "ax.grid()\n",
    "\n",
    "ax = axs[1]\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "ax.set_xlabel('Observed SLR')\n",
    "ax.set_ylabel('Predicted SLR')\n",
    "ax.plot([0, maxslr], [0, maxslr])\n",
    "ax.set_xlim([0, maxslr])\n",
    "ax.set_ylim([0, maxslr])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "axs[1].set_title('Elastic Model\\nR2: %.3f\\nMAE: %.3f'%(elastic_test_score, elastic_mae))\n",
    "\n",
    "ax = axs[2]\n",
    "error = test_predictions - y_test\n",
    "ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "ax.set_xlabel('Prediction Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid()\n",
    "\n",
    "# fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to use feature selection, we can do so here.\n",
    "Can choose the model from which to take features, do so manually, or not at all (set equal to X_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Top Predictor: %s'%elastic_keys[0])\n",
    "# fig, axs = plt.subplots(3, 3, figsize=(18, 18), facecolor='w')\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# for i, ax in enumerate(axs):\n",
    "#     ik, ic = elastic_keys[i], elastic_coefs[i]\n",
    "#     ax.scatter(X_train_norm[ik], y_train, c='k', marker='+', s=65, linewidth=0.5)\n",
    "#     ax.set_title('(%d: %.2f) %s'%(i+1, ic, ik))\n",
    "#     ax.grid()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_low_import = False\n",
    "\n",
    "# if drop_low_import:\n",
    "#     feature_selection = elastic_keys[mask]\n",
    "#     feature_selection\n",
    "\n",
    "#     pre_select = X_train_norm.keys()\n",
    "#     X_train_norm = X_train_norm[feature_selection]\n",
    "#     X_test_norm = X_test_norm[feature_selection]\n",
    "#     post_select = X_train_norm.keys()\n",
    "\n",
    "#     print('dropped:', [k for k in pre_select if k not in post_select])\n",
    "\n",
    "#     print(X_train_norm.shape)\n",
    "#     print(X_test_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "for layer_sizes in [1000]:\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(layer_sizes,), \n",
    "        activation='relu', \n",
    "        solver='sgd', \n",
    "        alpha=0.00001, \n",
    "        batch_size='auto', \n",
    "        learning_rate='adaptive', \n",
    "        learning_rate_init=0.01, \n",
    "        power_t=0.5, \n",
    "        max_iter=50000, \n",
    "        shuffle=True, \n",
    "        random_state=random_state, \n",
    "        tol=0.0001, \n",
    "        verbose=False, \n",
    "        warm_start=False, \n",
    "        momentum=0.7, \n",
    "        nesterovs_momentum=True, \n",
    "        early_stopping=True, \n",
    "        validation_fraction=0.3, \n",
    "        beta_1=0.5, \n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-8)\n",
    "\n",
    "    mlp.fit(X_train_norm, y_train)\n",
    "\n",
    "    mlp_train_score = mlp.score(X_train_norm, y_train)\n",
    "    mlp_test_score = mlp.score(X_test_norm, y_test)\n",
    "    \n",
    "    # mlp_coeff_used = np.sum(mlp.coef_ != 0)\n",
    "\n",
    "    print('Layer Sizes: %d'%layer_sizes)\n",
    "    print('MLP Train Score: %.3f'%mlp_train_score)\n",
    "    print('MLP Test Score: %.3f'%mlp_test_score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = mlp.predict(X_test_norm)\n",
    "mlp_mae = np.nanmean(abs(test_predictions - y_test))\n",
    "\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8), facecolor='w')\n",
    "\n",
    "ax = axs[0]\n",
    "maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "ax.grid()\n",
    "\n",
    "ax = axs[1]\n",
    "maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "maxslr += 5\n",
    "ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "ax.set_xlabel('Observed SLR')\n",
    "ax.set_ylabel('Predicted SLR')\n",
    "ax.plot([0, maxslr], [0, maxslr])\n",
    "ax.set_xlim([0, maxslr])\n",
    "ax.set_ylim([0, maxslr])\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "axs[1].set_title('R2: %.3f\\nMAE: %.3f'%(mlp_test_score, mlp_mae))\n",
    "\n",
    "ax = axs[2]\n",
    "error = test_predictions - y_test\n",
    "ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "ax.set_xlabel('Prediction Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid()\n",
    "\n",
    "fig.suptitle('MultiLayer Perceptron (Simple ANN) [%d Hidden Layer Sizes]'%layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Support Vector Regression Model\n",
    "(Other ML models like ANN from keras, tensorflow, pytorch may be used in this block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a K-Fold Cross Validation to assess model performance (Optional, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the SVR model multiprocess friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def SVR_mp(_params):\n",
    "    \n",
    "    print('.', end='')\n",
    "    \n",
    "    _i, _j, _C, _e = _params\n",
    "    \n",
    "    _model = SVR(\n",
    "                C=_C, #Ridge regularization parameter for (L2)^2 penalty\n",
    "                epsilon=_e, #Specifies the epsilon-tube within which no penalty is associated in the training loss function\n",
    "                kernel='rbf', #'linear', 'polynomial', 'rbf'\n",
    "                degree=3, #pass interger for 'polynomial' kernel, ignored otherwise\n",
    "                tol=0.001, #stopping tolerance\n",
    "                shrinking=False, \n",
    "                cache_size=200, \n",
    "                verbose=False)\n",
    "    \n",
    "    _model.fit(X_train_norm, y_train)\n",
    "    \n",
    "    test_predictions = _model.predict(X_test_norm).flatten()\n",
    "    _r2 = _model.score(X_test_norm, y_test) #sklearn.metrics.r2_score(y_test.values.flatten(), test_predictions)\n",
    "    _mse = sklearn.metrics.mean_squared_error(y_test.values.flatten(), test_predictions)\n",
    "    _mae = sklearn.metrics.mean_absolute_error(y_test.values.flatten(), test_predictions)\n",
    "    _mare = MARE(y_test.values.flatten(), test_predictions)\n",
    "    \n",
    "    return (_i, _j, _C, _e, _r2, _mae, _mse, _mare, _model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from sklearn.svm import SVR\n",
    "import sklearn\n",
    "\n",
    "# Best test\n",
    "crange = np.arange(1, 100, 1)\n",
    "erange = np.arange(1.0, 5.1, .25)\n",
    "\n",
    "# crange = np.arange(1, 25, 1)\n",
    "# erange = np.arange(0.1, 3.1, .1)\n",
    "\n",
    "params = {}\n",
    "params['r2'] = np.zeros((len(crange), len(erange)))\n",
    "params['mae'] = np.zeros((len(crange), len(erange)))\n",
    "params['mse'] = np.zeros((len(crange), len(erange)))\n",
    "params['mare'] = np.zeros((len(crange), len(erange)))\n",
    "params['model'] = np.empty((len(crange), len(erange)), dtype='object')\n",
    "params['epsilon'] = np.zeros((len(crange), len(erange)))\n",
    "params['C'] = np.zeros((len(crange), len(erange)))\n",
    "\n",
    "mp_params = np.array([[(i, j, C, e) for j, e in enumerate(erange)] \n",
    "                      for i, C in enumerate(crange)]).reshape(-1, 4)\n",
    "\n",
    "print('Iterations to attempt: %d'%len(mp_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVR, collect output iterated over hyperparameters (C, epsilon)<br>\n",
    "Specify the cost function/loss function used, tolerances, kernel, error metric, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paralellize here\n",
    "# mp_returns = [SVR_mp(_param) for _param in mp_params[:10]]\n",
    "\n",
    "with mp.get_context('fork').Pool(64) as p:\n",
    "    mp_returns = p.map(SVR_mp, mp_params, chunksize=1)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "for item in mp_returns:\n",
    "    \n",
    "    i, j, C, e, r2, mae, mse, mare, model = item\n",
    "    i, j = int(i), int(j)\n",
    "        \n",
    "    params['r2'][i, j] = r2\n",
    "    params['mse'][i, j] = mse\n",
    "    params['mae'][i, j] = mae\n",
    "    params['mare'][i, j] = mare\n",
    "    params['model'][i, j] = model\n",
    "    params['epsilon'][i, j] = e\n",
    "    params['C'][i, j] = C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model performance over time, cost/loss function evolution and skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot SVR output with hyperparameters (C, epsilon)<br>\n",
    "Apply a grid-search method to select best performing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_on, indexer, _ = 'R2', np.where(params['r2'] == params['r2'].max()), params['r2'].max()\n",
    "min_on, indexer, _ = 'MAE', np.where(params['mae'] == params['mae'].min()), params['mae'].min()\n",
    "min_on, indexer, _ = 'MSE', np.where(params['mse'] == params['mse'].min()), params['mse'].min()\n",
    "min_on, indexer, _ = 'MARE', np.where(params['mare'] == params['mare'].min()), params['mare'].min()\n",
    "\n",
    "for min_on in ['r2']:\n",
    "    \n",
    "    if min_on in ['mse', 'mae', 'mare']:\n",
    "        min_max = 'Minimized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].min())\n",
    "    elif min_on in ['r2']:\n",
    "        min_max = 'Maximized'\n",
    "        indexer = np.where(params[min_on] == params[min_on].max())\n",
    "\n",
    "    r, c = indexer\n",
    "    r, c = r[0], c[0]\n",
    "    r, c, _\n",
    "\n",
    "    model = params['model'][r, c]\n",
    "    test_predictions = model.predict(X_test_norm)\n",
    "    \n",
    "    y_true = y_test\n",
    "    y_pred = test_predictions\n",
    "    print('MARE ', MARE(y_true, y_pred))\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, facecolor='w', figsize=(24, 14))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    ax = axs[0]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mae'])\n",
    "    plt.colorbar(cbar, label='mae', ax=ax)\n",
    "    ax.set_title('Min MAE: %.3f'%params['mae'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[1]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['mse'])\n",
    "    plt.colorbar(cbar, label='mse', ax=ax)\n",
    "    ax.set_title('Min MSE: %.3f'%params['mse'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='w', marker='+')\n",
    "\n",
    "    ax = axs[2]\n",
    "    cbar = ax.pcolormesh(erange, crange, params['r2'])\n",
    "    plt.colorbar(cbar, label='r2', ax=ax)\n",
    "    ax.set_title('Max R^2: %.3f'%params['r2'][r, c])\n",
    "    ax.scatter(params['epsilon'][r, c], params['C'][r, c], s=500, c='k', marker='+')\n",
    "\n",
    "    for ax in axs[:3]:\n",
    "        ax.set_xlabel('epsilon')\n",
    "        ax.set_ylabel('C_val')\n",
    "        ax.set_ylim([crange.min(), crange.max()])\n",
    "        ax.set_xlim([erange.min(), erange.max()])\n",
    "\n",
    "    ax = axs[3]\n",
    "    maxslr = y_test.max() if y_test.max() > y_train.max() else y_train.max()\n",
    "\n",
    "    ax.hist(y_train, bins=np.arange(0, maxslr, 2), color='g', edgecolor='k', alpha=1.0, label='Train SLR\\nn=%d'%len(y_train))\n",
    "    ax.hist(y_test, bins=np.arange(0, maxslr, 2), color='C0', edgecolor='k', alpha=1.0, label='Test SLR\\nn=%d'%len(y_test))\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xticks(np.arange(0, maxslr+1, 5))\n",
    "    ax.set_xticklabels(np.arange(0, maxslr+1, 5).astype(int))\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[4]\n",
    "    maxslr = test_predictions.max() if test_predictions.max() > y_test.max() else y_test.max()\n",
    "    maxslr += 5\n",
    "    ax.scatter(y_test, test_predictions, c='k', s=50, marker='+', linewidth=0.75)\n",
    "    ax.set_xlabel('Observed SLR')\n",
    "    ax.set_ylabel('Predicted SLR')\n",
    "    ax.plot([0, maxslr], [0, maxslr])\n",
    "    ax.set_xlim([0, maxslr])\n",
    "    ax.set_ylim([0, maxslr])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid()\n",
    "\n",
    "    ax = axs[5]\n",
    "    error = test_predictions - y_test\n",
    "    ax.hist(error, bins=np.arange(-30, 30, 2), edgecolor='k')\n",
    "    ax.set_xlabel('Prediction Error')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid()\n",
    "\n",
    "    plt.suptitle('Support Vector Regression Model\\n%s\\n%s on: %s\\nepsilon %.3f\\nc_val: %.3f'%(site_list, min_max, min_on.upper(), params['epsilon'][r, c], params['C'][r, c]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe final trained SVR model\n",
    "Descriptive plots of predictor coefficient rank/influence<br>\n",
    "Print out model parameters, error metrics, skill scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranksort = np.argsort(abs(model.coef_))[::-1]\n",
    "# svr_coefs = model.coef_[0][ranksort][0]\n",
    "# svr_keys = X_train_norm.keys()[ranksort][0]\n",
    "# mask = svr_coefs != 0\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, facecolor='w', figsize=(8, 14))\n",
    "\n",
    "# ax.axvline(0, color='k', linewidth=3, zorder=11)\n",
    "# ax.barh(svr_keys[mask], svr_coefs[mask], color='green', zorder=10, height=0.97)\n",
    "\n",
    "# for i, k in enumerate(svr_keys):\n",
    "#     if svr_coefs[i] != 0:\n",
    "#         ax.text(svr_coefs[i]/2, k, k, zorder=20)\n",
    "\n",
    "# # ax.invert_yaxis()\n",
    "# ax.axes.get_yaxis().set_visible(False)\n",
    "# ax.set_title('SVR Coefs')\n",
    "# ax.set_xlabel('SVR Coef Value')\n",
    "# ax.grid(zorder=-1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out a usable model along with the input parameters for application\n",
    "Save with a descriptive filename and a metadata text file!<br>\n",
    "This will make swapping out models for evaluation much simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
