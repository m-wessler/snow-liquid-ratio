{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'CLN'\n",
    "# f = glob(obdir + '%s*.csv'%site)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate precipitating (snow) periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these out to a file later\n",
    "# precip_periods = data_auto[(\n",
    "#     (data_auto['snow_auto_mm'] > 0.) & \n",
    "#     (data_auto['swe_auto_mm'] > 0.) &\n",
    "#     (data_auto['tsfc_c'] <= 6.))].index\n",
    "\n",
    "# precip_periods3h = data_auto3h[(\n",
    "#     (data_auto3h['snow_auto_mm'] > 0.) & \n",
    "#     (data_auto3h['swe_auto_mm'] > 0.) &\n",
    "#     (data_auto3h['tsfc_c'] <= 6.))].index\n",
    "\n",
    "# precip_periods[:5], precip_periods3h[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the heavy lifting: extract a profile from the ERA5 and combine with the observation data\n",
    "This will allow us to have our predictor set and target variable in one dataset to train a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine ERA5 lat/lon gridpoint\n",
    "Import the metadata file and take what's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = glob(obdir + '*Metadata*.xlsx')[0]\n",
    "metadata = pd.read_excel(meta_file).set_index('code').loc[site]\n",
    "\n",
    "# Determine the start, end years from data if metadata is nan\n",
    "start, end = metadata['start'], metadata['end']\n",
    "start = data_save.index[0].year if np.isnan(start) else start\n",
    "start = data_save.index[0].year if data_save.index[0].year < start else start\n",
    "end = data_save.index[-1].year if np.isnan(end) else end\n",
    "end = data_save.index[-1].year if data_save.index[-1].year > end else end\n",
    "metadata['start'], metadata['end'] = start, end\n",
    "\n",
    "# Determine the lat, lon of the site from the metadata\n",
    "site_lat, site_lon = metadata['lat'], metadata['lon']\n",
    "site_elev = metadata['elevation_m']\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see if a profile for this lat/lon exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the era5_orog file to check the lat/lon grid\n",
    "era_dir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/era5/'\n",
    "era5_orog_file = era_dir + 'era5_orog.nc'\n",
    "era5_orog = xr.open_dataset(era5_orog_file)['z'].isel(time=0)\n",
    "era5_orog = era5_orog.rename({'latitude':'lat', 'longitude':'lon'})\n",
    "era5_lat, era5_lon = era5_orog['lat'], era5_orog['lon']\n",
    "\n",
    "# Find the index of the correct lat lon\n",
    "idx1d = (np.abs(era5_lon - site_lon) + np.abs(era5_lat - site_lat))\n",
    "idx = np.unravel_index(np.argmin(idx1d, axis=None), idx1d.shape)\n",
    "\n",
    "# Subset and convert gpm to m\n",
    "era5_g = 9.80665\n",
    "era5_orog = era5_orog.isel(lat=idx[1], lon=idx[0])/era5_g\n",
    "\n",
    "era5_orog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string based on the lat and lon of the nearest era5 gridpoint\n",
    "era5_prof_file = 'era5prof_{}N_{}W.nc'.format(\n",
    "    era5_orog['lat'].values, abs(era5_orog['lon'].values))\n",
    "\n",
    "print(era_dir + era5_prof_file)\n",
    "\n",
    "if isfile(era_dir + 'profiles/' + era5_prof_file):\n",
    "    # open profile\n",
    "    era5_prof = xr.open_dataset(era_dir + 'profiles/' + era5_prof_file)\n",
    "    \n",
    "    # check start, end \n",
    "    # if start, end outside of bounds, produce the missing years\n",
    "    # call the profile script with arguments for lat, lon, start, end\n",
    "    # concatenate the variables for new years, save output\n",
    "    # concatenate new years with old years, save output\n",
    "    \n",
    "else:\n",
    "    print('no')\n",
    "    # if profile is not found produce new\n",
    "    # call the profile script with arguments for lat, lon, start, end\n",
    "    # concatenate the variables, save output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task List:\n",
    "#### Import profile data in the aggregate\n",
    "#### Export the compiled data as netCDF\n",
    "#### Now all training gridpoints/sites will be in a consistent format and easy to aggregate!\n",
    "We can either expand the level variables here and calculate derived variables now or in a later script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
