{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import metpy.constants as mpconsts\n",
    "\n",
    "from glob import glob\n",
    "from os.path import isfile\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import gaussian_kde, skew\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and acquire list of available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/observations/'\n",
    "profdir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/era5/profiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build into a loop over all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob(obdir + 'clean/*.pd')\n",
    "\n",
    "# This can be a manual site list if desired\n",
    "site_list = [f.split('/')[-1].split('_')[0] for f in flist]\n",
    "site_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for site in site_list:\n",
    "site = 'BSNFJE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_file = glob(obdir + '*Metadata*.xlsx')[0]\n",
    "metadata = pd.read_excel(meta_file).set_index('code').loc[site]\n",
    "\n",
    "# Determine the lat, lon of the site from the metadata\n",
    "site_lat, site_lon = metadata['lat'], metadata['lon']\n",
    "site_elev = metadata['elevation_m']\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the ERA5 Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the era5_orog file to check the lat/lon grid\n",
    "era_dir = '/uufs/chpc.utah.edu/common/home/steenburgh-group10/mewessler/era5/'\n",
    "era5_orog_file = era_dir + 'era5_orog.nc'\n",
    "era5_orog = xr.open_dataset(era5_orog_file)['z'].isel(time=0)\n",
    "era5_orog = era5_orog.rename({'latitude':'lat', 'longitude':'lon'})\n",
    "era5_lat, era5_lon = era5_orog['lat'], era5_orog['lon']\n",
    "\n",
    "# Find the index of the correct lat lon\n",
    "idx1d = (np.abs(era5_lon - site_lon) + np.abs(era5_lat - site_lat))\n",
    "idx = np.unravel_index(np.argmin(idx1d, axis=None), idx1d.shape)\n",
    "\n",
    "# Subset and convert gpm to m\n",
    "era5_g = 9.80665\n",
    "era5_orog = era5_orog.isel(lat=idx[1], lon=idx[0])/era5_g\n",
    "\n",
    "era5_lat, era5_lon = era5_orog['lat'].values, era5_orog['lon'].values\n",
    "era5_elev = era5_orog.values\n",
    "\n",
    "print('ERA5 Profile:\\n%.2f %.2f %.2f\\n'%(era5_lat, era5_lon, era5_elev))\n",
    "\n",
    "era5_prof_file = 'era5prof_{}N_{}W.nc'.format(\n",
    "    era5_orog['lat'].values, abs(era5_orog['lon'].values))\n",
    "\n",
    "if isfile(era_dir + 'profiles/' + era5_prof_file):\n",
    "    era5_prof = xr.open_dataset(era_dir + 'profiles/' + era5_prof_file).load()\n",
    "    era5_prof = era5_prof.dropna(dim='time')\n",
    "    print(era5_prof)\n",
    "    \n",
    "else:\n",
    "    print('ERA5 Profile Missing:\\n%s'%era5_prof_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the surface observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    obs = pd.read_pickle(glob(obdir + 'clean/%s*.pd'%site)[0]).reset_index().rename(\n",
    "        columns={'index':'time'}).set_index('time')\n",
    "except:\n",
    "    obs = pd.read_pickle(glob(obdir + 'clean/%s*.pd'%site)[0]).reset_index().rename(\n",
    "        columns={'datetime_utc':'time'}).set_index('time')\n",
    "\n",
    "obs = obs[obs.index > datetime(1980, 8, 1)]\n",
    "obs = obs.dropna()\n",
    "\n",
    "plt.figure(facecolor='w')\n",
    "plt.hist([t.hour for t in obs.index], bins=np.arange(0, 24, 1))\n",
    "plt.grid()\n",
    "plt.title('Hour of Observation')\n",
    "plt.show()\n",
    "\n",
    "obs[:10]\n",
    "\n",
    "intervals = [int(k.replace('slr', '')) for k in obs if 'slr' in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune observations based on minimum qpf, snow, and max T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('12h Data Used to Constrain SWE/SNOW')\n",
    "    obs = obs[((obs['swe12_mm'] > 1.4) & (obs['snow12_mm'] >= 25.5))]\n",
    "except:\n",
    "    print('24h Data Used to Constrain SWE/SNOW')\n",
    "    obs = obs[((obs['swe24_mm'] > 2.8) & (obs['snow24_mm'] >= 51.0))]\n",
    "print(obs.shape)\n",
    "\n",
    "try:\n",
    "    obs = obs.drop_duplicates()\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    print(obs.shape)\n",
    "\n",
    "obs = obs.iloc[np.where(np.array([len(pp) for pp in obs['precip_periods']]) > 0)[0]]\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the data with a few plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ycolors = (\"#704D9E\",\"#704D9E\",\"#7A4EA0\",\"#824EA3\",\"#8B4FA4\",\"#9350A6\",\"#9B51A7\",\"#A253A8\",\"#AA54A9\",\n",
    "           \"#B156A9\",\"#B758A9\",\"#BE5AA9\",\"#C45DA8\",\"#CA60A7\",\"#CF63A6\",\"#D566A4\",\"#DA6AA3\",\"#DE6DA1\",\n",
    "           \"#E3719E\",\"#E7759C\",\"#EB7A99\",\"#EE7F96\",\"#F18493\",\"#F28A90\",\"#F48F8D\",\"#F5958B\",\"#F79A88\",\n",
    "           \"#F7A086\", \"#F8A684\",\"#F9AB83\",\"#F9B182\",\"#F9B681\",\"#F9BC81\",\"#F9C182\",\"#F8C783\",\"#F8CC85\",\n",
    "           \"#F7D288\",\"#F6D78B\",\"#F5DC8F\",\"#F4E294\",\"#F3E79A\")\n",
    "\n",
    "binwidth = 2.5\n",
    "\n",
    "for k in [k for k in obs if 'slr' in k]:\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(24, 14), facecolor='w')\n",
    "    axs = axs.flatten()\n",
    "    ax1, ax2, ax3, ax4, ax5, ax6 = axs\n",
    "    \n",
    "    histargs = {'width':1, 'color':'0.5', 'edgecolor':'k', 'zorder':10}\n",
    "    vlineargs = {'linewidth':3, 'linestyle':'--', 'zorder':11}\n",
    "    xyargs = {'s':150, 'marker':'+', 'linewidth':1.5, 'zorder':10}\n",
    "    \n",
    "    v1, b1 = np.histogram(obs[k].values, bins=np.arange(0, 50+1, binwidth))    \n",
    "    ax1.bar(np.arange(.5, len(v1), 1), v1/sum(v1), label=k.upper(), **histargs)\n",
    "    vmean = obs[k].mean()\n",
    "    ax1.axvline(vmean/binwidth, c='green', label='Mean: %.2f'%vmean, **vlineargs)\n",
    "    vmed = obs[k].median()\n",
    "    ax1.axvline(vmed/binwidth, c='orange', label='Median: %.2f'%vmed, **vlineargs)\n",
    "    \n",
    "    ax1.set_xticks(np.arange(len(b1)))\n",
    "    ax1.set_xticklabels(b1, rotation=60)\n",
    "    ax1.set_xlabel(k.upper())\n",
    "    ax1.set_ylabel('FREQ_NORM')\n",
    "    \n",
    "    years = np.unique([t.year for t in obs.index])\n",
    "    nyears = len(years)\n",
    "    ygrays = np.linspace(.8, .4, nyears).astype(str)\n",
    "    \n",
    "    colid = np.linspace(0, len(ycolors)-1, nyears, dtype=int)\n",
    "    \n",
    "    for i, year in enumerate(np.unique([t.year for t in obs.index])):\n",
    "        \n",
    "        label = year if i==0 or i==nyears-1 else None\n",
    "        yobs = obs[k].iloc[[i for i, t in enumerate(obs.index) if t.year == year]]\n",
    "\n",
    "        try:\n",
    "            n, x = np.histogram(yobs, bins=np.arange(0, 50+1, 0.1))\n",
    "            ax2.plot(x, gaussian_kde(yobs)(x), color=ycolors[colid[i]], linewidth=2, label=label)\n",
    "\n",
    "            n, x = np.histogram(yobs, bins=np.arange(0, 50+1, 0.1))\n",
    "            ax3.plot(x, np.cumsum(gaussian_kde(yobs)(x))/10, color=ycolors[colid[i]], linewidth=2, label=label)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ax5.scatter(year, yobs.mean(), c='orange', **xyargs)\n",
    "        ax5.scatter(year, yobs.median(), c='green', **xyargs)        \n",
    "        ax6.scatter(year, np.std(yobs), c='purple', **xyargs)\n",
    "        ax6.scatter(year, skew(yobs), c='blue', **xyargs)\n",
    "        \n",
    "    ax5.scatter(year, -1, c='orange', label='Mean', **xyargs)\n",
    "    ax5.scatter(year, -1, c='green', label='Median', **xyargs)        \n",
    "    ax6.scatter(year, -1, c='purple', label='Standard Deviation', **xyargs)\n",
    "    ax6.scatter(year, -1, c='blue', label='Skewness', **xyargs)\n",
    "\n",
    "    n, x = np.histogram(yobs, bins=np.arange(0, 50+1, 0.1))\n",
    "    ax2.plot(x, gaussian_kde(obs[k])(x), 'k--', linewidth=2, label='All')\n",
    "    n, x = np.histogram(yobs, bins=np.arange(0, 50+1, 0.1))\n",
    "    ax3.plot(x, np.cumsum(gaussian_kde(obs[k])(x))/10, 'k--', linewidth=2, label='All')\n",
    "    \n",
    "    mobs = []\n",
    "    for i, month in enumerate([10, 11, 12, 1, 2, 3, 4, 5]):\n",
    "        mobs.append(obs[k].iloc[[i for i, t in enumerate(obs.index) if t.month == month]])\n",
    "        \n",
    "    bp = ax4.boxplot(mobs, showfliers=False, patch_artist=True, zorder=15)\n",
    "    [[box.set(linewidth=2, color='k', alpha=0.8) for box in bp[k]] \n",
    "         for k in ['boxes', 'whiskers', 'medians', 'means', 'caps']]\n",
    "    [patch.set_facecolor('0.5') for patch in bp['boxes']]\n",
    "    \n",
    "    ax4.axhline(vmean, c='green', label='Mean: %.2f'%vmean, **vlineargs)\n",
    "    ax4.axhline(vmed, c='orange', label='Median: %.2f'%vmed, **vlineargs)\n",
    "    \n",
    "    ax4.set_xlim(right=9)\n",
    "    ax4.set_ylim([0, 50])\n",
    "    ax4.set_xticklabels(['Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
    "    \n",
    "    ax5.set_ylim(top=30)\n",
    "    for ax in [ax5, ax6]:\n",
    "        ax.set_xlim([years[0], years[-1]])\n",
    "        ax.set_xticks(np.arange(years[0], years[-1], 1)[::2])\n",
    "        ax.set_xticklabels(np.arange(years[0], years[-1], 1)[::2], rotation=60)\n",
    "    \n",
    "    for ax in [ax2, ax3]:\n",
    "        ax.set_xlim(right=50)\n",
    "        ax.set_xlim(left=0)\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.set_ylim(bottom=0)\n",
    "        ax.legend()\n",
    "        ax.grid()\n",
    "    \n",
    "    plt.suptitle('%s %d – %d\\nn=%d'%(site, years[0], years[-1], obs.index.size))\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim the sub-surface levels out here\n",
    "Probably best to nan them out rather than get rid of the levels themselves for easier indexing later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_elev = era5_elev #metadata['elevation_m']\n",
    "use_min = False\n",
    "\n",
    "# If we want to use the GFS elevation we can import it here\n",
    "print('ERA5 Elevation: %dm\\nMeasured Elevation: %dm'%(\n",
    "    era5_elev, metadata['elevation_m']))\n",
    "\n",
    "zmean = era5_prof['Z'].mean(dim='time')/era5_g\n",
    "zmax = era5_prof['Z'].max(dim='time')/era5_g\n",
    "zmin = era5_prof['Z'].min(dim='time')/era5_g\n",
    "\n",
    "for level in era5_prof.level.values:\n",
    " \n",
    "    zcompare = zmean.sel(level=level)\n",
    "    zcompare1 = zmin.sel(level=level) if use_min else zcompare\n",
    "    \n",
    "    if ((zcompare >= trim_elev)&(zcompare1 >= trim_elev)):\n",
    "        lowest_level = level\n",
    "    \n",
    "    else:\n",
    "        print('\\nLowest Level Kept: %d mb\\n\\tMean Z: %.2f\\n\\tMax Z: %.2f\\n\\tMin Z: %.2f'%(\n",
    "            lowest_level, zmean.sel(level=lowest_level),\n",
    "            zmax.sel(level=lowest_level), zmin.sel(level=lowest_level)))\n",
    "        \n",
    "        print('\\nTrim Level: %d mb\\n\\tMean Z: %.2f\\n\\tMax Z: %.2f\\n\\tMin Z: %.2f'%(\n",
    "            level, zmean.sel(level=level),\n",
    "            zmax.sel(level=level), zmin.sel(level=level)))\n",
    "        \n",
    "        break\n",
    "\n",
    "# May want to consdier leaving the levels in and using a nan-fill if this\n",
    "# proves difficult to merge into a larger dataframe later\n",
    "era5_prof = era5_prof.sel(level=slice(era5_prof.level.min(), lowest_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care of the derived variables\n",
    "Wind Speed, Wind Direction<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level_set in ['', '10M', '100M']:\n",
    "    \n",
    "    u = era5_prof['U%s'%level_set] \n",
    "    v = era5_prof['V%s'%level_set]\n",
    "    \n",
    "    wspd = np.sqrt((u**2) + (v**2))\n",
    "    wdir = 90 - np.degrees(np.arctan2(-v, -u))\n",
    "    wdir = xr.where(wdir <= 0, wdir+360, wdir)\n",
    "    wdir = xr.where(((u == 0) & (v == 0)), 0, wdir)\n",
    "    \n",
    "    era5_prof['SPD%s'%level_set] = wspd.astype(np.float32)\n",
    "    era5_prof['DIR%s'%level_set] = wdir.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to observation interval\n",
    "From 1h or 3h ERA5 to 6, 12, 24h observations<br>\n",
    "I think we need to manually resample here...<br>\n",
    "to account for irregular observation intervals at some stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_these = {\n",
    "    'max':['Q', 'T', 'U', 'V', 'VO', 'W', 'Z', 'R', 'T2M', 'D2M', 'BLH', 'CAPE', 'MSL',\n",
    "           'SP', 'SPD', 'U10M', 'V10M', 'U100M', 'V100M', 'SPD10M', 'DIR10M', 'SPD100M', 'DIR100M'],\n",
    "    \n",
    "    'min':['Q', 'T', 'VO', 'Z', 'R', 'T2M', 'D2M', 'MSL', 'SP'],\n",
    "    \n",
    "    'mean':['Q', 'T', 'U', 'V', 'VO', 'W', 'Z', 'R', 'T2M', 'D2M', 'BLH', 'CAPE', 'MSL', \n",
    "           'SP', 'SPD', 'U10M', 'V10M', 'U100M', 'V100M', 'SPD10M', 'DIR10M', 'SPD100M', 'DIR100M']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interval in intervals:\n",
    "    \n",
    "    left_inclusive = True\n",
    "    right_inclusive = True\n",
    "\n",
    "    era5_resampled = []\n",
    "\n",
    "    for i, t in enumerate(obs.index):\n",
    "\n",
    "        # Determine start, end\n",
    "        ti = t - timedelta(hours=interval)    \n",
    "        ti = ti if left_inclusive else ti + timedelta(hours=1)\n",
    "        tf = t if right_inclusive else t - timedelta(hours=1)    \n",
    "        \n",
    "        try:\n",
    "            era5_slice = era5_prof.sel(time=obs.loc[t]['precip_periods'])\n",
    "        except:\n",
    "            era5_slice = era5_prof.sel(time=slice(ti, tf))\n",
    "\n",
    "        if era5_slice.time.size > 0:\n",
    "\n",
    "            print('\\rresampling %dh %d/%d (slice size %d)'%(interval, i+1, len(obs.index), era5_slice.time.size), end='')\n",
    "\n",
    "            slice_max = era5_slice[resample_these['max']].max(dim='time')\n",
    "            slice_min = era5_slice[resample_these['min']].min(dim='time')\n",
    "            slice_mean = era5_slice[resample_these['mean']].mean(dim='time')\n",
    "\n",
    "            slice_all = []\n",
    "            for oper in ['max', 'min', 'mean']:\n",
    "                slice_oper = vars()['slice_%s'%oper]\n",
    "\n",
    "                for k in slice_oper.keys():\n",
    "                    slice_oper = slice_oper.rename({k:'%s_%s'%(k, oper)})\n",
    "\n",
    "                    # Ensure that we label the observation time appropriately\n",
    "                    slice_oper['time'] = t\n",
    "\n",
    "                slice_all.append(slice_oper)\n",
    "\n",
    "            era5_resampled.append(xr.merge(slice_all))\n",
    "\n",
    "    era5_resampled = xr.concat(era5_resampled, dim='time').dropna(dim='time')\n",
    "    print()\n",
    "\n",
    "    ### Transform 2D vars (level, time) to 1D vars (time) for model input\n",
    "    isokeys = [k for k in era5_resampled if len(era5_resampled[k].shape) > 1]\n",
    "    sfckeys = [k for k in era5_resampled if len(era5_resampled[k].shape) == 1]\n",
    "\n",
    "    era5_df = []\n",
    "\n",
    "    for l in era5_resampled.level.values:\n",
    "\n",
    "        level_data = era5_resampled[isokeys].sel(level=l).drop('level')#.to_dataframe()\n",
    "        level_data = level_data.rename({k:'%s%d_%s'%(k.split('_')[0], l, k.split('_')[-1]) for k in level_data.keys()})\n",
    "\n",
    "        era5_df.append(level_data)\n",
    "\n",
    "    era5_df = xr.merge(era5_df)\n",
    "    era5_df = xr.merge([era5_df, era5_resampled[sfckeys]])\n",
    "\n",
    "    era5_df = era5_df.to_dataframe()\n",
    "    era5_df = era5_df.drop_duplicates().dropna().sort_index()\n",
    "\n",
    "    droplist = [k for k in obs.keys() if (('slr%d'%interval not in k)&('swe%d'%interval not in k))]\n",
    "\n",
    "    df_merged = obs.drop(columns=droplist).merge(era5_df, on='time', copy=False)\n",
    "    df_merged = df_merged.loc[np.in1d(era5_df.index, obs.index)]\n",
    "\n",
    "    print(interval, df_merged.shape)\n",
    "    df_merged.iloc[:10, :12]\n",
    "    \n",
    "    # df_save[interval] = df_merged.copy()\n",
    "    start, end = df_merged.index[0].year, df_merged.index[-1].year\n",
    "    df_savestr = obdir + 'combined/' + '%s_%04d_%04d.%02dh.pd'%(site, start, end, interval)\n",
    "    df_merged.to_pickle(df_savestr)\n",
    "    print('Saved:\\n%s'%df_savestr)\n",
    "#     del df_merged\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
